.\" Automatically generated by Pod::Man 2.27 (Pod::Simple 3.28)
.\"
.\" Standard preamble:
.\" ========================================================================
.de Sp \" Vertical space (when we can't use .PP)
.if t .sp .5v
.if n .sp
..
.de Vb \" Begin verbatim text
.ft CW
.nf
.ne \\$1
..
.de Ve \" End verbatim text
.ft R
.fi
..
.\" Set up some character translations and predefined strings.  \*(-- will
.\" give an unbreakable dash, \*(PI will give pi, \*(L" will give a left
.\" double quote, and \*(R" will give a right double quote.  \*(C+ will
.\" give a nicer C++.  Capital omega is used to do unbreakable dashes and
.\" therefore won't be available.  \*(C` and \*(C' expand to `' in nroff,
.\" nothing in troff, for use with C<>.
.tr \(*W-
.ds C+ C\v'-.1v'\h'-1p'\s-2+\h'-1p'+\s0\v'.1v'\h'-1p'
.ie n \{\
.    ds -- \(*W-
.    ds PI pi
.    if (\n(.H=4u)&(1m=24u) .ds -- \(*W\h'-12u'\(*W\h'-12u'-\" diablo 10 pitch
.    if (\n(.H=4u)&(1m=20u) .ds -- \(*W\h'-12u'\(*W\h'-8u'-\"  diablo 12 pitch
.    ds L" ""
.    ds R" ""
.    ds C` ""
.    ds C' ""
'br\}
.el\{\
.    ds -- \|\(em\|
.    ds PI \(*p
.    ds L" ``
.    ds R" ''
.    ds C`
.    ds C'
'br\}
.\"
.\" Escape single quotes in literal strings from groff's Unicode transform.
.ie \n(.g .ds Aq \(aq
.el       .ds Aq '
.\"
.\" If the F register is turned on, we'll generate index entries on stderr for
.\" titles (.TH), headers (.SH), subsections (.SS), items (.Ip), and index
.\" entries marked with X<> in POD.  Of course, you'll have to process the
.\" output yourself in some meaningful fashion.
.\"
.\" Avoid warning from groff about undefined register 'F'.
.de IX
..
.nr rF 0
.if \n(.g .if rF .nr rF 1
.if (\n(rF:(\n(.g==0)) \{
.    if \nF \{
.        de IX
.        tm Index:\\$1\t\\n%\t"\\$2"
..
.        if !\nF==2 \{
.            nr % 0
.            nr F 2
.        \}
.    \}
.\}
.rr rF
.\"
.\" Accent mark definitions (@(#)ms.acc 1.5 88/02/08 SMI; from UCB 4.2).
.\" Fear.  Run.  Save yourself.  No user-serviceable parts.
.    \" fudge factors for nroff and troff
.if n \{\
.    ds #H 0
.    ds #V .8m
.    ds #F .3m
.    ds #[ \f1
.    ds #] \fP
.\}
.if t \{\
.    ds #H ((1u-(\\\\n(.fu%2u))*.13m)
.    ds #V .6m
.    ds #F 0
.    ds #[ \&
.    ds #] \&
.\}
.    \" simple accents for nroff and troff
.if n \{\
.    ds ' \&
.    ds ` \&
.    ds ^ \&
.    ds , \&
.    ds ~ ~
.    ds /
.\}
.if t \{\
.    ds ' \\k:\h'-(\\n(.wu*8/10-\*(#H)'\'\h"|\\n:u"
.    ds ` \\k:\h'-(\\n(.wu*8/10-\*(#H)'\`\h'|\\n:u'
.    ds ^ \\k:\h'-(\\n(.wu*10/11-\*(#H)'^\h'|\\n:u'
.    ds , \\k:\h'-(\\n(.wu*8/10)',\h'|\\n:u'
.    ds ~ \\k:\h'-(\\n(.wu-\*(#H-.1m)'~\h'|\\n:u'
.    ds / \\k:\h'-(\\n(.wu*8/10-\*(#H)'\z\(sl\h'|\\n:u'
.\}
.    \" troff and (daisy-wheel) nroff accents
.ds : \\k:\h'-(\\n(.wu*8/10-\*(#H+.1m+\*(#F)'\v'-\*(#V'\z.\h'.2m+\*(#F'.\h'|\\n:u'\v'\*(#V'
.ds 8 \h'\*(#H'\(*b\h'-\*(#H'
.ds o \\k:\h'-(\\n(.wu+\w'\(de'u-\*(#H)/2u'\v'-.3n'\*(#[\z\(de\v'.3n'\h'|\\n:u'\*(#]
.ds d- \h'\*(#H'\(pd\h'-\w'~'u'\v'-.25m'\f2\(hy\fP\v'.25m'\h'-\*(#H'
.ds D- D\\k:\h'-\w'D'u'\v'-.11m'\z\(hy\v'.11m'\h'|\\n:u'
.ds th \*(#[\v'.3m'\s+1I\s-1\v'-.3m'\h'-(\w'I'u*2/3)'\s-1o\s+1\*(#]
.ds Th \*(#[\s+2I\s-2\h'-\w'I'u*3/5'\v'-.3m'o\v'.3m'\*(#]
.ds ae a\h'-(\w'a'u*4/10)'e
.ds Ae A\h'-(\w'A'u*4/10)'E
.    \" corrections for vroff
.if v .ds ~ \\k:\h'-(\\n(.wu*9/10-\*(#H)'\s-2\u~\d\s+2\h'|\\n:u'
.if v .ds ^ \\k:\h'-(\\n(.wu*10/11-\*(#H)'\v'-.4m'^\v'.4m'\h'|\\n:u'
.    \" for low resolution devices (crt and lpr)
.if \n(.H>23 .if \n(.V>19 \
\{\
.    ds : e
.    ds 8 ss
.    ds o a
.    ds d- d\h'-1'\(ga
.    ds D- D\h'-1'\(hy
.    ds th \o'bp'
.    ds Th \o'LP'
.    ds ae ae
.    ds Ae AE
.\}
.rm #[ #] #H #V #F C
.\" ========================================================================
.\"
.IX Title "DBIx::Class::Manual::Cookbook 3"
.TH DBIx::Class::Manual::Cookbook 3 "2014-01-22" "perl v5.18.4" "User Contributed Perl Documentation"
.\" For nroff, turn off justification.  Always turn off hyphenation; it makes
.\" way too many mistakes in technical documents.
.if n .ad l
.nh
.SH "NAME"
DBIx::Class::Manual::Cookbook \- Miscellaneous recipes
.SH "SEARCHING"
.IX Header "SEARCHING"
.SS "Paged results"
.IX Subsection "Paged results"
When you expect a large number of results, you can ask DBIx::Class for a
paged resultset, which will fetch only a defined number of records at a time:
.PP
.Vb 7
\&  my $rs = $schema\->resultset(\*(AqArtist\*(Aq)\->search(
\&    undef,
\&    {
\&      page => 1,  # page to return (defaults to 1)
\&      rows => 10, # number of results per page
\&    },
\&  );
\&
\&  return $rs\->all(); # all records for page 1
\&
\&  return $rs\->page(2); # records for page 2
.Ve
.PP
You can get a Data::Page object for the resultset (suitable for use
in e.g. a template) using the \f(CW\*(C`pager\*(C'\fR method:
.PP
.Vb 1
\&  return $rs\->pager();
.Ve
.SS "Complex \s-1WHERE\s0 clauses"
.IX Subsection "Complex WHERE clauses"
Sometimes you need to formulate a query using specific operators:
.PP
.Vb 4
\&  my @albums = $schema\->resultset(\*(AqAlbum\*(Aq)\->search({
\&    artist => { \*(Aqlike\*(Aq, \*(Aq%Lamb%\*(Aq },
\&    title  => { \*(Aqlike\*(Aq, \*(Aq%Fear of Fours%\*(Aq },
\&  });
.Ve
.PP
This results in something like the following \f(CW\*(C`WHERE\*(C'\fR clause:
.PP
.Vb 1
\&  WHERE artist LIKE ? AND title LIKE ?
.Ve
.PP
And the following bind values for the placeholders: \f(CW\*(Aq%Lamb%\*(Aq\fR, \f(CW\*(Aq%Fear of
Fours%\*(Aq\fR.
.PP
Other queries might require slightly more complex logic:
.PP
.Vb 9
\&  my @albums = $schema\->resultset(\*(AqAlbum\*(Aq)\->search({
\&    \-or => [
\&      \-and => [
\&        artist => { \*(Aqlike\*(Aq, \*(Aq%Smashing Pumpkins%\*(Aq },
\&        title  => \*(AqSiamese Dream\*(Aq,
\&      ],
\&      artist => \*(AqStarchildren\*(Aq,
\&    ],
\&  });
.Ve
.PP
This results in the following \f(CW\*(C`WHERE\*(C'\fR clause:
.PP
.Vb 2
\&  WHERE ( artist LIKE \*(Aq%Smashing Pumpkins%\*(Aq AND title = \*(AqSiamese Dream\*(Aq )
\&    OR artist = \*(AqStarchildren\*(Aq
.Ve
.PP
For more information on generating complex queries, see
\&\*(L"\s-1WHERE CLAUSES\*(R"\s0 in SQL::Abstract.
.SS "Retrieve one and only one row from a resultset"
.IX Subsection "Retrieve one and only one row from a resultset"
Sometimes you need only the first \*(L"top\*(R" row of a resultset. While this
can be easily done with \f(CW$rs\fR\->first, it is suboptimal, as a full blown cursor for the resultset will be
created and then immediately destroyed after fetching the first row
object.  \f(CW$rs\fR\->single is designed
specifically for this case \- it will grab the first returned result
without even instantiating a cursor.
.PP
Before replacing all your calls to \f(CW\*(C`first()\*(C'\fR with \f(CW\*(C`single()\*(C'\fR please observe the
following \s-1CAVEATS:\s0
.IP "\(bu" 4
While \fIsingle()\fR takes a search condition just like \fIsearch()\fR does, it does
_not_ accept search attributes. However one can always chain a \fIsingle()\fR to
a \fIsearch()\fR:
.Sp
.Vb 1
\&  my $top_cd = $cd_rs\->search({}, { order_by => \*(Aqrating\*(Aq })\->single;
.Ve
.IP "\(bu" 4
Since \fIsingle()\fR is the engine behind \fIfind()\fR, it is designed to fetch a
single row per database query. Thus a warning will be issued when the
underlying \s-1SELECT\s0 returns more than one row. Sometimes however this usage
is valid: i.e. we have an arbitrary number of cd's but only one of them is
at the top of the charts at any given time. If you know what you are doing,
you can silence the warning by explicitly limiting the resultset size:
.Sp
.Vb 1
\&  my $top_cd = $cd_rs\->search ({}, { order_by => \*(Aqrating\*(Aq, rows => 1 })\->single;
.Ve
.SS "Arbitrary \s-1SQL\s0 through a custom ResultSource"
.IX Subsection "Arbitrary SQL through a custom ResultSource"
Sometimes you have to run arbitrary \s-1SQL\s0 because your query is too complex
(e.g. it contains Unions, Sub-Selects, Stored Procedures, etc.) or has to
be optimized for your database in a special way, but you still want to
get the results as a DBIx::Class::ResultSet.
.PP
This is accomplished by defining a
ResultSource::View for your query,
almost like you would define a regular ResultSource.
.PP
.Vb 4
\&  package My::Schema::Result::UserFriendsComplex;
\&  use strict;
\&  use warnings;
\&  use base qw/DBIx::Class::Core/;
\&
\&  _\|_PACKAGE_\|_\->table_class(\*(AqDBIx::Class::ResultSource::View\*(Aq);
\&
\&  # For the time being this is necessary even for virtual views
\&  _\|_PACKAGE_\|_\->table($view_name);
\&
\&  #
\&  # \->add_columns, etc.
\&  #
\&
\&  # do not attempt to deploy() this view
\&  _\|_PACKAGE_\|_\->result_source_instance\->is_virtual(1);
\&
\&  _\|_PACKAGE_\|_\->result_source_instance\->view_definition(q[
\&    SELECT u.* FROM user u
\&    INNER JOIN user_friends f ON u.id = f.user_id
\&    WHERE f.friend_user_id = ?
\&    UNION
\&    SELECT u.* FROM user u
\&    INNER JOIN user_friends f ON u.id = f.friend_user_id
\&    WHERE f.user_id = ?
\&  ]);
.Ve
.PP
Next, you can execute your complex query using bind parameters like this:
.PP
.Vb 5
\&  my $friends = $schema\->resultset( \*(AqUserFriendsComplex\*(Aq )\->search( {},
\&    {
\&      bind  => [ 12345, 12345 ]
\&    }
\&  );
.Ve
.PP
\&... and you'll get back a perfect DBIx::Class::ResultSet (except, of course,
that you cannot modify the rows it contains, e.g. cannot call \*(L"update\*(R",
\&\*(L"delete\*(R", ...  on it).
.PP
Note that you cannot have bind parameters unless is_virtual is set to true.
.IP "\(bu" 4
\&\s-1NOTE\s0
.Sp
If you're using the old deprecated \f(CW\*(C`$rsrc_instance\->name(\e\*(Aq( SELECT ...\*(Aq)\*(C'\fR
method for custom \s-1SQL\s0 execution, you are highly encouraged to update your code
to use a virtual view as above. If you do not want to change your code, and just
want to suppress the deprecation warning when you call
\&\*(L"deploy\*(R" in DBIx::Class::Schema, add this line to your source definition, so that
\&\f(CW\*(C`deploy\*(C'\fR will exclude this \*(L"table\*(R":
.Sp
.Vb 1
\&  sub sqlt_deploy_hook { $_[1]\->schema\->drop_table ($_[1]) }
.Ve
.SS "Using specific columns"
.IX Subsection "Using specific columns"
When you only want specific columns from a table, you can use
\&\f(CW\*(C`columns\*(C'\fR to specify which ones you need. This is useful to avoid
loading columns with large amounts of data that you aren't about to
use anyway:
.PP
.Vb 6
\&  my $rs = $schema\->resultset(\*(AqArtist\*(Aq)\->search(
\&    undef,
\&    {
\&      columns => [qw/ name /]
\&    }
\&  );
\&
\&  # Equivalent SQL:
\&  # SELECT artist.name FROM artist
.Ve
.PP
This is a shortcut for \f(CW\*(C`select\*(C'\fR and \f(CW\*(C`as\*(C'\fR, see below. \f(CW\*(C`columns\*(C'\fR
cannot be used together with \f(CW\*(C`select\*(C'\fR and \f(CW\*(C`as\*(C'\fR.
.SS "Using database functions or stored procedures"
.IX Subsection "Using database functions or stored procedures"
The combination of \f(CW\*(C`select\*(C'\fR and \f(CW\*(C`as\*(C'\fR can be used to return the result of a
database function or stored procedure as a column value. You use \f(CW\*(C`select\*(C'\fR to
specify the source for your column value (e.g. a column name, function, or
stored procedure name). You then use \f(CW\*(C`as\*(C'\fR to set the column name you will use
to access the returned value:
.PP
.Vb 7
\&  my $rs = $schema\->resultset(\*(AqArtist\*(Aq)\->search(
\&    {},
\&    {
\&      select => [ \*(Aqname\*(Aq, { LENGTH => \*(Aqname\*(Aq } ],
\&      as     => [qw/ name name_length /],
\&    }
\&  );
\&
\&  # Equivalent SQL:
\&  # SELECT name name, LENGTH( name )
\&  # FROM artist
.Ve
.PP
Note that the \f(CW\*(C`as\*(C'\fR attribute \fBhas absolutely nothing to do\fR with the \s-1SQL\s0
syntax \f(CW\*(C` SELECT foo AS bar \*(C'\fR (see the documentation in
\&\*(L"\s-1ATTRIBUTES\*(R"\s0 in DBIx::Class::ResultSet). You can control the \f(CW\*(C`AS\*(C'\fR part of the
generated \s-1SQL\s0 via the \f(CW\*(C`\-as\*(C'\fR field attribute as follows:
.PP
.Vb 10
\&  my $rs = $schema\->resultset(\*(AqArtist\*(Aq)\->search(
\&    {},
\&    {
\&      join => \*(Aqcds\*(Aq,
\&      distinct => 1,
\&      \*(Aq+select\*(Aq => [ { count => \*(Aqcds.cdid\*(Aq, \-as => \*(Aqamount_of_cds\*(Aq } ],
\&      \*(Aq+as\*(Aq => [qw/num_cds/],
\&      order_by => { \-desc => \*(Aqamount_of_cds\*(Aq },
\&    }
\&  );
\&
\&  # Equivalent SQL
\&  # SELECT me.artistid, me.name, me.rank, me.charfield, COUNT( cds.cdid ) AS amount_of_cds
\&  #   FROM artist me LEFT JOIN cd cds ON cds.artist = me.artistid
\&  # GROUP BY me.artistid, me.name, me.rank, me.charfield
\&  # ORDER BY amount_of_cds DESC
.Ve
.PP
If your alias exists as a column in your base class (i.e. it was added with
add_columns), you just access it as
normal. Our \f(CW\*(C`Artist\*(C'\fR class has a \f(CW\*(C`name\*(C'\fR column, so we just use the \f(CW\*(C`name\*(C'\fR
accessor:
.PP
.Vb 2
\&  my $artist = $rs\->first();
\&  my $name = $artist\->name();
.Ve
.PP
If on the other hand the alias does not correspond to an existing column, you
have to fetch the value using the \f(CW\*(C`get_column\*(C'\fR accessor:
.PP
.Vb 1
\&  my $name_length = $artist\->get_column(\*(Aqname_length\*(Aq);
.Ve
.PP
If you don't like using \f(CW\*(C`get_column\*(C'\fR, you can always create an accessor for
any of your aliases using either of these:
.PP
.Vb 2
\&  # Define accessor manually:
\&  sub name_length { shift\->get_column(\*(Aqname_length\*(Aq); }
\&
\&  # Or use DBIx::Class::AccessorGroup:
\&  _\|_PACKAGE_\|_\->mk_group_accessors(\*(Aqcolumn\*(Aq => \*(Aqname_length\*(Aq);
.Ve
.PP
See also \*(L"Using \s-1SQL\s0 functions on the left hand side of a comparison\*(R".
.SS "\s-1SELECT DISTINCT\s0 with multiple columns"
.IX Subsection "SELECT DISTINCT with multiple columns"
.Vb 7
\&  my $rs = $schema\->resultset(\*(AqArtist\*(Aq)\->search(
\&    {},
\&    {
\&      columns => [ qw/artist_id name rank/ ],
\&      distinct => 1
\&    }
\&  );
\&
\&  my $rs = $schema\->resultset(\*(AqArtist\*(Aq)\->search(
\&    {},
\&    {
\&      columns => [ qw/artist_id name rank/ ],
\&      group_by => [ qw/artist_id name rank/ ],
\&    }
\&  );
\&
\&  # Equivalent SQL:
\&  # SELECT me.artist_id, me.name, me.rank
\&  # FROM artist me
\&  # GROUP BY artist_id, name, rank
.Ve
.SS "\s-1SELECT COUNT\s0(\s-1DISTINCT\s0 colname)"
.IX Subsection "SELECT COUNT(DISTINCT colname)"
.Vb 7
\&  my $rs = $schema\->resultset(\*(AqArtist\*(Aq)\->search(
\&    {},
\&    {
\&      columns => [ qw/name/ ],
\&      distinct => 1
\&    }
\&  );
\&
\&  my $rs = $schema\->resultset(\*(AqArtist\*(Aq)\->search(
\&    {},
\&    {
\&      columns => [ qw/name/ ],
\&      group_by => [ qw/name/ ],
\&    }
\&  );
\&
\&  my $count = $rs\->count;
\&
\&  # Equivalent SQL:
\&  # SELECT COUNT( * ) FROM (SELECT me.name FROM artist me GROUP BY me.name) me:
.Ve
.SS "Grouping results"
.IX Subsection "Grouping results"
DBIx::Class supports \f(CW\*(C`GROUP BY\*(C'\fR as follows:
.PP
.Vb 9
\&  my $rs = $schema\->resultset(\*(AqArtist\*(Aq)\->search(
\&    {},
\&    {
\&      join     => [qw/ cds /],
\&      select   => [ \*(Aqname\*(Aq, { count => \*(Aqcds.id\*(Aq } ],
\&      as       => [qw/ name cd_count /],
\&      group_by => [qw/ name /]
\&    }
\&  );
\&
\&  # Equivalent SQL:
\&  # SELECT name, COUNT( cd.id ) FROM artist
\&  # LEFT JOIN cd ON artist.id = cd.artist
\&  # GROUP BY name
.Ve
.PP
Please see \*(L"\s-1ATTRIBUTES\*(R"\s0 in DBIx::Class::ResultSet documentation if you
are in any way unsure about the use of the attributes above (\f(CW\*(C` join
\&\*(C'\fR, \f(CW\*(C` select \*(C'\fR, \f(CW\*(C` as \*(C'\fR and \f(CW\*(C` group_by \*(C'\fR).
.SS "Subqueries"
.IX Subsection "Subqueries"
You can write subqueries relatively easily in \s-1DBIC.\s0
.PP
.Vb 3
\&  my $inside_rs = $schema\->resultset(\*(AqArtist\*(Aq)\->search({
\&    name => [ \*(AqBilly Joel\*(Aq, \*(AqBrittany Spears\*(Aq ],
\&  });
\&
\&  my $rs = $schema\->resultset(\*(AqCD\*(Aq)\->search({
\&    artist_id => { \-in => $inside_rs\->get_column(\*(Aqid\*(Aq)\->as_query },
\&  });
.Ve
.PP
The usual operators ( '=', '!=', \-in, \-not_in, etc.) are supported.
.PP
\&\fB\s-1NOTE\s0\fR: You have to explicitly use '=' when doing an equality comparison.
The following will \fBnot\fR work:
.PP
.Vb 3
\&  my $rs = $schema\->resultset(\*(AqCD\*(Aq)\->search({
\&    artist_id => $inside_rs\->get_column(\*(Aqid\*(Aq)\->as_query,  # does NOT work
\&  });
.Ve
.PP
\fISupport\fR
.IX Subsection "Support"
.PP
Subqueries are supported in the where clause (first hashref), and in the
from, select, and +select attributes.
.PP
\fICorrelated subqueries\fR
.IX Subsection "Correlated subqueries"
.PP
.Vb 9
\&  my $cdrs = $schema\->resultset(\*(AqCD\*(Aq);
\&  my $rs = $cdrs\->search({
\&    year => {
\&      \*(Aq=\*(Aq => $cdrs\->search(
\&        { artist_id => { \-ident => \*(Aqme.artist_id\*(Aq } },
\&        { alias => \*(Aqsub_query\*(Aq }
\&      )\->get_column(\*(Aqyear\*(Aq)\->max_rs\->as_query,
\&    },
\&  });
.Ve
.PP
That creates the following \s-1SQL:\s0
.PP
.Vb 7
\&  SELECT me.cdid, me.artist, me.title, me.year, me.genreid, me.single_track
\&    FROM cd me
\&  WHERE year = (
\&    SELECT MAX(sub_query.year)
\&      FROM cd sub_query
\&    WHERE artist_id = me.artist_id
\&  )
.Ve
.SS "Predefined searches"
.IX Subsection "Predefined searches"
You can define frequently used searches as methods by subclassing
DBIx::Class::ResultSet:
.PP
.Vb 4
\&  package My::DBIC::ResultSet::CD;
\&  use strict;
\&  use warnings;
\&  use base \*(AqDBIx::Class::ResultSet\*(Aq;
\&
\&  sub search_cds_ordered {
\&      my ($self) = @_;
\&
\&      return $self\->search(
\&          {},
\&          { order_by => \*(Aqname DESC\*(Aq },
\&      );
\&  }
\&
\&  1;
.Ve
.PP
If you're using \*(L"load_namespaces\*(R" in DBIx::Class::Schema, simply place the file
into the \f(CW\*(C`ResultSet\*(C'\fR directory next to your \f(CW\*(C`Result\*(C'\fR directory, and it will
be automatically loaded.
.PP
If however you are still using \*(L"load_classes\*(R" in DBIx::Class::Schema, first tell
DBIx::Class to create an instance of the ResultSet class for you, in your
My::DBIC::Schema::CD class:
.PP
.Vb 3
\&  # class definition as normal
\&  use base \*(AqDBIx::Class::Core\*(Aq;
\&  _\|_PACKAGE_\|_\->table(\*(Aqcd\*(Aq);
\&
\&  # tell DBIC to use the custom ResultSet class
\&  _\|_PACKAGE_\|_\->resultset_class(\*(AqMy::DBIC::ResultSet::CD\*(Aq);
.Ve
.PP
Note that \f(CW\*(C`resultset_class\*(C'\fR must be called after \f(CW\*(C`load_components\*(C'\fR and \f(CW\*(C`table\*(C'\fR, or you will get errors about missing methods.
.PP
Then call your new method in your code:
.PP
.Vb 1
\&   my $ordered_cds = $schema\->resultset(\*(AqCD\*(Aq)\->search_cds_ordered();
.Ve
.SS "Using \s-1SQL\s0 functions on the left hand side of a comparison"
.IX Subsection "Using SQL functions on the left hand side of a comparison"
Using \s-1SQL\s0 functions on the left hand side of a comparison is generally not a
good idea since it requires a scan of the entire table. (Unless your \s-1RDBMS\s0
supports indexes on expressions \- including return values of functions \- and
you create an index on the return value of the function in question.) However,
it can be accomplished with \f(CW\*(C`DBIx::Class\*(C'\fR when necessary by resorting to
literal \s-1SQL:\s0
.PP
.Vb 3
\&  $rs\->search(
\&    \e[ \*(AqYEAR(date_of_birth) = ?\*(Aq, 1979 ]
\&  );
\&
\&  # Equivalent SQL:
\&  # SELECT * FROM employee WHERE YEAR(date_of_birth) = ?
.Ve
.PP
To include the function as part of a larger search, use the '\-and' keyword
to collect the search conditions:
.PP
.Vb 4
\&  $rs\->search({ \-and => [
\&    name => \*(AqBob\*(Aq,
\&    \e[ \*(AqYEAR(date_of_birth) = ?\*(Aq, 1979 ]
\&  ]});
\&
\&  # Equivalent SQL:
\&  # SELECT * FROM employee WHERE name = ? AND YEAR(date_of_birth) = ?
.Ve
.PP
Note: the syntax for specifying the bind value's datatype and value is
explained in \*(L"\s-1DBIC BIND VALUES\*(R"\s0 in DBIx::Class::ResultSet.
.PP
See also \*(L"Literal \s-1SQL\s0 with placeholders and bind values
(subqueries)\*(R" in SQL::Abstract.
.SS "Software Limits"
.IX Subsection "Software Limits"
When your \s-1RDBMS\s0 does not have a working \s-1SQL\s0 limit mechanism (e.g. Sybase \s-1ASE\s0)
and GenericSubQ is either too slow or does
not work at all, you can try the
software_limit
DBIx::Class::ResultSet attribute, which skips over records to simulate limits
in the Perl layer.
.PP
For example:
.PP
.Vb 6
\&  my $paged_rs = $rs\->search({}, {
\&    rows => 25,
\&    page => 3,
\&    order_by => [ \*(Aqme.last_name\*(Aq ],
\&    software_limit => 1,
\&  });
.Ve
.PP
You can set it as a default for your schema by placing the following in your
\&\f(CW\*(C`Schema.pm\*(C'\fR:
.PP
.Vb 1
\&  _\|_PACKAGE_\|_\->default_resultset_attributes({ software_limit => 1 });
.Ve
.PP
\&\fB\s-1WARNING:\s0\fR If you are dealing with large resultsets and your \s-1DBI\s0 or
\&\s-1ODBC/ADO\s0 driver does not have proper cursor support (i.e. it loads the whole
resultset into memory) then this feature will be extremely slow and use huge
amounts of memory at best, and may cause your process to run out of memory and
cause instability on your server at worst, beware!
.SH "JOINS AND PREFETCHING"
.IX Header "JOINS AND PREFETCHING"
.SS "Using joins and prefetch"
.IX Subsection "Using joins and prefetch"
You can use the \f(CW\*(C`join\*(C'\fR attribute to allow searching on, or sorting your
results by, one or more columns in a related table.
.PP
This requires that you have defined the DBIx::Class::Relationship. For example :
.PP
.Vb 1
\&  My::Schema::CD\->has_many( artists => \*(AqMy::Schema::Artist\*(Aq, \*(Aqartist_id\*(Aq);
.Ve
.PP
To return all CDs matching a particular artist name, you specify the name of the relationship ('artists'):
.PP
.Vb 8
\&  my $rs = $schema\->resultset(\*(AqCD\*(Aq)\->search(
\&    {
\&      \*(Aqartists.name\*(Aq => \*(AqBob Marley\*(Aq
\&    },
\&    {
\&      join => \*(Aqartists\*(Aq, # join the artist table
\&    }
\&  );
\&
\&  # Equivalent SQL:
\&  # SELECT cd.* FROM cd
\&  # JOIN artist ON cd.artist = artist.id
\&  # WHERE artist.name = \*(AqBob Marley\*(Aq
.Ve
.PP
In that example both the join, and the condition use the relationship name rather than the table name
(see DBIx::Class::Manual::Joining for more details on aliasing ).
.PP
If required, you can now sort on any column in the related tables by including
it in your \f(CW\*(C`order_by\*(C'\fR attribute, (again using the aliased relation name rather than table name) :
.PP
.Vb 9
\&  my $rs = $schema\->resultset(\*(AqCD\*(Aq)\->search(
\&    {
\&      \*(Aqartists.name\*(Aq => \*(AqBob Marley\*(Aq
\&    },
\&    {
\&      join     => \*(Aqartists\*(Aq,
\&      order_by => [qw/ artists.name /]
\&    }
\&  );
\&
\&  # Equivalent SQL:
\&  # SELECT cd.* FROM cd
\&  # JOIN artist ON cd.artist = artist.id
\&  # WHERE artist.name = \*(AqBob Marley\*(Aq
\&  # ORDER BY artist.name
.Ve
.PP
Note that the \f(CW\*(C`join\*(C'\fR attribute should only be used when you need to search or
sort using columns in a related table. Joining related tables when you only
need columns from the main table will make performance worse!
.PP
Now let's say you want to display a list of CDs, each with the name of the
artist. The following will work fine:
.PP
.Vb 3
\&  while (my $cd = $rs\->next) {
\&    print "CD: " . $cd\->title . ", Artist: " . $cd\->artist\->name;
\&  }
.Ve
.PP
There is a problem however. We have searched both the \f(CW\*(C`cd\*(C'\fR and \f(CW\*(C`artist\*(C'\fR tables
in our main query, but we have only returned data from the \f(CW\*(C`cd\*(C'\fR table. To get
the artist name for any of the \s-1CD\s0 objects returned, DBIx::Class will go back
to the database:
.PP
.Vb 1
\&  SELECT artist.* FROM artist WHERE artist.id = ?
.Ve
.PP
A statement like the one above will run for each and every \s-1CD\s0 returned by our
main query. Five CDs, five extra queries. A hundred CDs, one hundred extra
queries!
.PP
Thankfully, DBIx::Class has a \f(CW\*(C`prefetch\*(C'\fR attribute to solve this problem.
This allows you to fetch results from related tables in advance:
.PP
.Vb 10
\&  my $rs = $schema\->resultset(\*(AqCD\*(Aq)\->search(
\&    {
\&      \*(Aqartists.name\*(Aq => \*(AqBob Marley\*(Aq
\&    },
\&    {
\&      join     => \*(Aqartists\*(Aq,
\&      order_by => [qw/ artists.name /],
\&      prefetch => \*(Aqartists\*(Aq # return artist data too!
\&    }
\&  );
\&
\&  # Equivalent SQL (note SELECT from both "cd" and "artist"):
\&  # SELECT cd.*, artist.* FROM cd
\&  # JOIN artist ON cd.artist = artist.id
\&  # WHERE artist.name = \*(AqBob Marley\*(Aq
\&  # ORDER BY artist.name
.Ve
.PP
The code to print the \s-1CD\s0 list remains the same:
.PP
.Vb 3
\&  while (my $cd = $rs\->next) {
\&    print "CD: " . $cd\->title . ", Artist: " . $cd\->artist\->name;
\&  }
.Ve
.PP
DBIx::Class has now prefetched all matching data from the \f(CW\*(C`artist\*(C'\fR table,
so no additional \s-1SQL\s0 statements are executed. You now have a much more
efficient query.
.PP
Also note that \f(CW\*(C`prefetch\*(C'\fR should only be used when you know you will
definitely use data from a related table. Pre-fetching related tables when you
only need columns from the main table will make performance worse!
.SS "Multiple joins"
.IX Subsection "Multiple joins"
In the examples above, the \f(CW\*(C`join\*(C'\fR attribute was a scalar.  If you
pass an array reference instead, you can join to multiple tables.  In
this example, we want to limit the search further, using
\&\f(CW\*(C`LinerNotes\*(C'\fR:
.PP
.Vb 10
\&  # Relationships defined elsewhere:
\&  # CD\->belongs_to(\*(Aqartist\*(Aq => \*(AqArtist\*(Aq);
\&  # CD\->has_one(\*(Aqliner_notes\*(Aq => \*(AqLinerNotes\*(Aq, \*(Aqcd\*(Aq);
\&  my $rs = $schema\->resultset(\*(AqCD\*(Aq)\->search(
\&    {
\&      \*(Aqartist.name\*(Aq => \*(AqBob Marley\*(Aq
\&      \*(Aqliner_notes.notes\*(Aq => { \*(Aqlike\*(Aq, \*(Aq%some text%\*(Aq },
\&    },
\&    {
\&      join     => [qw/ artist liner_notes /],
\&      order_by => [qw/ artist.name /],
\&    }
\&  );
\&
\&  # Equivalent SQL:
\&  # SELECT cd.*, artist.*, liner_notes.* FROM cd
\&  # JOIN artist ON cd.artist = artist.id
\&  # JOIN liner_notes ON cd.id = liner_notes.cd
\&  # WHERE artist.name = \*(AqBob Marley\*(Aq AND liner_notes.notes LIKE \*(Aq%some text%\*(Aq
\&  # ORDER BY artist.name
.Ve
.SS "Multi-step joins"
.IX Subsection "Multi-step joins"
Sometimes you want to join more than one relationship deep. In this example,
we want to find all \f(CW\*(C`Artist\*(C'\fR objects who have \f(CW\*(C`CD\*(C'\fRs whose \f(CW\*(C`LinerNotes\*(C'\fR
contain a specific string:
.PP
.Vb 3
\&  # Relationships defined elsewhere:
\&  # Artist\->has_many(\*(Aqcds\*(Aq => \*(AqCD\*(Aq, \*(Aqartist\*(Aq);
\&  # CD\->has_one(\*(Aqliner_notes\*(Aq => \*(AqLinerNotes\*(Aq, \*(Aqcd\*(Aq);
\&
\&  my $rs = $schema\->resultset(\*(AqArtist\*(Aq)\->search(
\&    {
\&      \*(Aqliner_notes.notes\*(Aq => { \*(Aqlike\*(Aq, \*(Aq%some text%\*(Aq },
\&    },
\&    {
\&      join => {
\&        \*(Aqcds\*(Aq => \*(Aqliner_notes\*(Aq
\&      }
\&    }
\&  );
\&
\&  # Equivalent SQL:
\&  # SELECT artist.* FROM artist
\&  # LEFT JOIN cd ON artist.id = cd.artist
\&  # LEFT JOIN liner_notes ON cd.id = liner_notes.cd
\&  # WHERE liner_notes.notes LIKE \*(Aq%some text%\*(Aq
.Ve
.PP
Joins can be nested to an arbitrary level. So if we decide later that we
want to reduce the number of Artists returned based on who wrote the liner
notes:
.PP
.Vb 2
\&  # Relationship defined elsewhere:
\&  # LinerNotes\->belongs_to(\*(Aqauthor\*(Aq => \*(AqPerson\*(Aq);
\&
\&  my $rs = $schema\->resultset(\*(AqArtist\*(Aq)\->search(
\&    {
\&      \*(Aqliner_notes.notes\*(Aq => { \*(Aqlike\*(Aq, \*(Aq%some text%\*(Aq },
\&      \*(Aqauthor.name\*(Aq => \*(AqA. Writer\*(Aq
\&    },
\&    {
\&      join => {
\&        \*(Aqcds\*(Aq => {
\&          \*(Aqliner_notes\*(Aq => \*(Aqauthor\*(Aq
\&        }
\&      }
\&    }
\&  );
\&
\&  # Equivalent SQL:
\&  # SELECT artist.* FROM artist
\&  # LEFT JOIN cd ON artist.id = cd.artist
\&  # LEFT JOIN liner_notes ON cd.id = liner_notes.cd
\&  # LEFT JOIN author ON author.id = liner_notes.author
\&  # WHERE liner_notes.notes LIKE \*(Aq%some text%\*(Aq
\&  # AND author.name = \*(AqA. Writer\*(Aq
.Ve
.SS "Multi-step and multiple joins"
.IX Subsection "Multi-step and multiple joins"
With various combinations of array and hash references, you can join
tables in any combination you desire.  For example, to join Artist to
\&\s-1CD\s0 and Concert, and join \s-1CD\s0 to LinerNotes:
.PP
.Vb 2
\&  # Relationships defined elsewhere:
\&  # Artist\->has_many(\*(Aqconcerts\*(Aq => \*(AqConcert\*(Aq, \*(Aqartist\*(Aq);
\&
\&  my $rs = $schema\->resultset(\*(AqArtist\*(Aq)\->search(
\&    { },
\&    {
\&      join => [
\&        {
\&          cds => \*(Aqliner_notes\*(Aq
\&        },
\&        \*(Aqconcerts\*(Aq
\&      ],
\&    }
\&  );
\&
\&  # Equivalent SQL:
\&  # SELECT artist.* FROM artist
\&  # LEFT JOIN cd ON artist.id = cd.artist
\&  # LEFT JOIN liner_notes ON cd.id = liner_notes.cd
\&  # LEFT JOIN concert ON artist.id = concert.artist
.Ve
.SS "Multi-step prefetch"
.IX Subsection "Multi-step prefetch"
\&\f(CW\*(C`prefetch\*(C'\fR can be nested more than one relationship
deep using the same syntax as a multi-step join:
.PP
.Vb 8
\&  my $rs = $schema\->resultset(\*(AqTag\*(Aq)\->search(
\&    {},
\&    {
\&      prefetch => {
\&        cd => \*(Aqartist\*(Aq
\&      }
\&    }
\&  );
\&
\&  # Equivalent SQL:
\&  # SELECT tag.*, cd.*, artist.* FROM tag
\&  # JOIN cd ON tag.cd = cd.id
\&  # JOIN artist ON cd.artist = artist.id
.Ve
.PP
Now accessing our \f(CW\*(C`cd\*(C'\fR and \f(CW\*(C`artist\*(C'\fR relationships does not need additional
\&\s-1SQL\s0 statements:
.PP
.Vb 2
\&  my $tag = $rs\->first;
\&  print $tag\->cd\->artist\->name;
.Ve
.SH "ROW-LEVEL OPERATIONS"
.IX Header "ROW-LEVEL OPERATIONS"
.SS "Retrieving a result object's Schema"
.IX Subsection "Retrieving a result object's Schema"
It is possible to get a Schema object from a result object like so:
.PP
.Vb 3
\&  my $schema = $cd\->result_source\->schema;
\&  # use the schema as normal:
\&  my $artist_rs = $schema\->resultset(\*(AqArtist\*(Aq);
.Ve
.PP
This can be useful when you don't want to pass around a Schema object to every
method.
.SS "Getting the value of the primary key for the last database insert"
.IX Subsection "Getting the value of the primary key for the last database insert"
\&\s-1AKA\s0 getting last_insert_id
.PP
Thanks to the core component PK::Auto, this is straightforward:
.PP
.Vb 3
\&  my $foo = $rs\->create(\e%blah);
\&  # do more stuff
\&  my $id = $foo\->id; # foo\->my_primary_key_field will also work.
.Ve
.PP
If you are not using autoincrementing primary keys, this will probably
not work, but then you already know the value of the last primary key anyway.
.SS "Stringification"
.IX Subsection "Stringification"
Employ the standard stringification technique by using the overload
module.
.PP
To make an object stringify itself as a single column, use something
like this (replace \f(CW\*(C`name\*(C'\fR with the column/method of your choice):
.PP
.Vb 1
\&  use overload \*(Aq""\*(Aq => sub { shift\->name}, fallback => 1;
.Ve
.PP
For more complex stringification, you can use an anonymous subroutine:
.PP
.Vb 2
\&  use overload \*(Aq""\*(Aq => sub { $_[0]\->name . ", " .
\&                             $_[0]\->address }, fallback => 1;
.Ve
.PP
\fIStringification Example\fR
.IX Subsection "Stringification Example"
.PP
Suppose we have two tables: \f(CW\*(C`Product\*(C'\fR and \f(CW\*(C`Category\*(C'\fR. The table
specifications are:
.PP
.Vb 2
\&  Product(id, Description, category)
\&  Category(id, Description)
.Ve
.PP
\&\f(CW\*(C`category\*(C'\fR is a foreign key into the Category table.
.PP
If you have a Product object \f(CW$obj\fR and write something like
.PP
.Vb 1
\&  print $obj\->category
.Ve
.PP
things will not work as expected.
.PP
To obtain, for example, the category description, you should add this
method to the class defining the Category table:
.PP
.Vb 2
\&  use overload "" => sub {
\&      my $self = shift;
\&
\&      return $self\->Description;
\&  }, fallback => 1;
.Ve
.SS "Want to know if find_or_create found or created a row?"
.IX Subsection "Want to know if find_or_create found or created a row?"
Just use \f(CW\*(C`find_or_new\*(C'\fR instead, then check \f(CW\*(C`in_storage\*(C'\fR:
.PP
.Vb 5
\&  my $obj = $rs\->find_or_new({ blah => \*(Aqblarg\*(Aq });
\&  unless ($obj\->in_storage) {
\&    $obj\->insert;
\&    # do whatever else you wanted if it was a new row
\&  }
.Ve
.SS "Static sub-classing DBIx::Class result classes"
.IX Subsection "Static sub-classing DBIx::Class result classes"
\&\s-1AKA\s0 adding additional relationships/methods/etc. to a model for a
specific usage of the (shared) model.
.PP
\&\fBSchema definition\fR
.PP
.Vb 1
\&    package My::App::Schema;
\&
\&    use base \*(AqDBIx::Class::Schema\*(Aq;
\&
\&    # load subclassed classes from My::App::Schema::Result/ResultSet
\&    _\|_PACKAGE_\|_\->load_namespaces;
\&
\&    # load classes from shared model
\&    load_classes({
\&        \*(AqMy::Shared::Model::Result\*(Aq => [qw/
\&            Foo
\&            Bar
\&        /]});
\&
\&    1;
.Ve
.PP
\&\fBResult-Subclass definition\fR
.PP
.Vb 1
\&    package My::App::Schema::Result::Baz;
\&
\&    use strict;
\&    use warnings;
\&    use base \*(AqMy::Shared::Model::Result::Baz\*(Aq;
\&
\&    # WARNING: Make sure you call table() again in your subclass,
\&    # otherwise DBIx::Class::ResultSourceProxy::Table will not be called
\&    # and the class name is not correctly registered as a source
\&    _\|_PACKAGE_\|_\->table(\*(Aqbaz\*(Aq);
\&
\&    sub additional_method {
\&        return "I\*(Aqm an additional method only needed by this app";
\&    }
\&
\&    1;
.Ve
.SS "Dynamic Sub-classing DBIx::Class proxy classes"
.IX Subsection "Dynamic Sub-classing DBIx::Class proxy classes"
\&\s-1AKA\s0 multi-class object inflation from one table
.PP
DBIx::Class classes are proxy classes, therefore some different
techniques need to be employed for more than basic subclassing.  In
this example we have a single user table that carries a boolean bit
for admin.  We would like to give the admin users
objects (DBIx::Class::Row) the same methods as a regular user but
also special admin only methods.  It doesn't make sense to create two
separate proxy-class files for this.  We would be copying all the user
methods into the Admin class.  There is a cleaner way to accomplish
this.
.PP
Overriding the \f(CW\*(C`inflate_result\*(C'\fR method within the User proxy-class
gives us the effect we want.  This method is called by
DBIx::Class::ResultSet when inflating a result from storage.  So we
grab the object being returned, inspect the values we are looking for,
bless it if it's an admin object, and then return it.  See the example
below:
.PP
\&\fBSchema Definition\fR
.PP
.Vb 1
\&    package My::Schema;
\&
\&    use base qw/DBIx::Class::Schema/;
\&
\&    _\|_PACKAGE_\|_\->load_namespaces;
\&
\&    1;
.Ve
.PP
\&\fBProxy-Class definitions\fR
.PP
.Vb 1
\&    package My::Schema::Result::User;
\&
\&    use strict;
\&    use warnings;
\&    use base qw/DBIx::Class::Core/;
\&
\&    ### Define what our admin class is, for ensure_class_loaded()
\&    my $admin_class = _\|_PACKAGE_\|_ . \*(Aq::Admin\*(Aq;
\&
\&    _\|_PACKAGE_\|_\->table(\*(Aqusers\*(Aq);
\&
\&    _\|_PACKAGE_\|_\->add_columns(qw/user_id   email    password
\&                                firstname lastname active
\&                                admin/);
\&
\&    _\|_PACKAGE_\|_\->set_primary_key(\*(Aquser_id\*(Aq);
\&
\&    sub inflate_result {
\&        my $self = shift;
\&        my $ret = $self\->next::method(@_);
\&        if( $ret\->admin ) {### If this is an admin, rebless for extra functions
\&            $self\->ensure_class_loaded( $admin_class );
\&            bless $ret, $admin_class;
\&        }
\&        return $ret;
\&    }
\&
\&    sub hello {
\&        print "I am a regular user.\en";
\&        return ;
\&    }
\&
\&    1;
\&
\&
\&    package My::Schema::Result::User::Admin;
\&
\&    use strict;
\&    use warnings;
\&    use base qw/My::Schema::Result::User/;
\&
\&    # This line is important
\&    _\|_PACKAGE_\|_\->table(\*(Aqusers\*(Aq);
\&
\&    sub hello
\&    {
\&        print "I am an admin.\en";
\&        return;
\&    }
\&
\&    sub do_admin_stuff
\&    {
\&        print "I am doing admin stuff\en";
\&        return ;
\&    }
\&
\&    1;
.Ve
.PP
\&\fBTest File\fR test.pl
.PP
.Vb 3
\&    use warnings;
\&    use strict;
\&    use My::Schema;
\&
\&    my $user_data = { email    => \*(Aqsomeguy@place.com\*(Aq,
\&                      password => \*(Aqpass1\*(Aq,
\&                      admin    => 0 };
\&
\&    my $admin_data = { email    => \*(Aqsomeadmin@adminplace.com\*(Aq,
\&                       password => \*(Aqpass2\*(Aq,
\&                       admin    => 1 };
\&
\&    my $schema = My::Schema\->connection(\*(Aqdbi:Pg:dbname=test\*(Aq);
\&
\&    $schema\->resultset(\*(AqUser\*(Aq)\->create( $user_data );
\&    $schema\->resultset(\*(AqUser\*(Aq)\->create( $admin_data );
\&
\&    ### Now we search for them
\&    my $user = $schema\->resultset(\*(AqUser\*(Aq)\->single( $user_data );
\&    my $admin = $schema\->resultset(\*(AqUser\*(Aq)\->single( $admin_data );
\&
\&    print ref $user, "\en";
\&    print ref $admin, "\en";
\&
\&    print $user\->password , "\en"; # pass1
\&    print $admin\->password , "\en";# pass2; inherited from User
\&    print $user\->hello , "\en";# I am a regular user.
\&    print $admin\->hello, "\en";# I am an admin.
\&
\&    ### The statement below will NOT print
\&    print "I can do admin stuff\en" if $user\->can(\*(Aqdo_admin_stuff\*(Aq);
\&    ### The statement below will print
\&    print "I can do admin stuff\en" if $admin\->can(\*(Aqdo_admin_stuff\*(Aq);
.Ve
.PP
Alternatively you can use DBIx::Class::DynamicSubclass that implements
exactly the above functionality.
.SS "Skip result object creation for faster results"
.IX Subsection "Skip result object creation for faster results"
DBIx::Class is not built for speed, it's built for convenience and
ease of use, but sometimes you just need to get the data, and skip the
fancy objects.
.PP
To do this simply use DBIx::Class::ResultClass::HashRefInflator.
.PP
.Vb 1
\& my $rs = $schema\->resultset(\*(AqCD\*(Aq);
\&
\& $rs\->result_class(\*(AqDBIx::Class::ResultClass::HashRefInflator\*(Aq);
\&
\& my $hash_ref = $rs\->find(1);
.Ve
.PP
Wasn't that easy?
.PP
Beware, changing the Result class using
\&\*(L"result_class\*(R" in DBIx::Class::ResultSet will replace any existing class
completely including any special components loaded using
load_components, eg DBIx::Class::InflateColumn::DateTime.
.SS "Get raw data for blindingly fast results"
.IX Subsection "Get raw data for blindingly fast results"
If the HashRefInflator solution
above is not fast enough for you, you can use a DBIx::Class to return values
exactly as they come out of the database with none of the convenience methods
wrapped round them.
.PP
This is used like so:
.PP
.Vb 4
\&  my $cursor = $rs\->cursor
\&  while (my @vals = $cursor\->next) {
\&      # use $val[0..n] here
\&  }
.Ve
.PP
You will need to map the array offsets to particular columns (you can
use the \*(L"select\*(R" in DBIx::Class::ResultSet attribute of \*(L"search\*(R" in DBIx::Class::ResultSet to force ordering).
.SH "RESULTSET OPERATIONS"
.IX Header "RESULTSET OPERATIONS"
.SS "Getting Schema from a ResultSet"
.IX Subsection "Getting Schema from a ResultSet"
To get the DBIx::Class::Schema object from a ResultSet, do the following:
.PP
.Vb 1
\& $rs\->result_source\->schema
.Ve
.SS "Getting Columns Of Data"
.IX Subsection "Getting Columns Of Data"
\&\s-1AKA\s0 Aggregating Data
.PP
If you want to find the sum of a particular column there are several
ways, the obvious one is to use search:
.PP
.Vb 8
\&  my $rs = $schema\->resultset(\*(AqItems\*(Aq)\->search(
\&    {},
\&    {
\&       select => [ { sum => \*(AqCost\*(Aq } ],
\&       as     => [ \*(Aqtotal_cost\*(Aq ], # remember this \*(Aqas\*(Aq is for DBIx::Class::ResultSet not SQL
\&    }
\&  );
\&  my $tc = $rs\->first\->get_column(\*(Aqtotal_cost\*(Aq);
.Ve
.PP
Or, you can use the DBIx::Class::ResultSetColumn, which gets
returned when you ask the \f(CW\*(C`ResultSet\*(C'\fR for a column using
\&\f(CW\*(C`get_column\*(C'\fR:
.PP
.Vb 2
\&  my $cost = $schema\->resultset(\*(AqItems\*(Aq)\->get_column(\*(AqCost\*(Aq);
\&  my $tc = $cost\->sum;
.Ve
.PP
With this you can also do:
.PP
.Vb 2
\&  my $minvalue = $cost\->min;
\&  my $maxvalue = $cost\->max;
.Ve
.PP
Or just iterate through the values of this column only:
.PP
.Vb 3
\&  while ( my $c = $cost\->next ) {
\&    print $c;
\&  }
\&
\&  foreach my $c ($cost\->all) {
\&    print $c;
\&  }
.Ve
.PP
\&\f(CW\*(C`ResultSetColumn\*(C'\fR only has a limited number of built-in functions. If
you need one that it doesn't have, then you can use the \f(CW\*(C`func\*(C'\fR method
instead:
.PP
.Vb 1
\&  my $avg = $cost\->func(\*(AqAVERAGE\*(Aq);
.Ve
.PP
This will cause the following \s-1SQL\s0 statement to be run:
.PP
.Vb 1
\&  SELECT AVERAGE(Cost) FROM Items me
.Ve
.PP
Which will of course only work if your database supports this function.
See DBIx::Class::ResultSetColumn for more documentation.
.SS "Creating a result set from a set of rows"
.IX Subsection "Creating a result set from a set of rows"
Sometimes you have a (set of) result objects that you want to put into a
resultset without the need to hit the \s-1DB\s0 again. You can do that by using the
set_cache method:
.PP
.Vb 9
\& my @uploadable_groups;
\& while (my $group = $groups\->next) {
\&   if ($group\->can_upload($self)) {
\&     push @uploadable_groups, $group;
\&   }
\& }
\& my $new_rs = $self\->result_source\->resultset;
\& $new_rs\->set_cache(\e@uploadable_groups);
\& return $new_rs;
.Ve
.SH "USING RELATIONSHIPS"
.IX Header "USING RELATIONSHIPS"
.SS "Create a new row in a related table"
.IX Subsection "Create a new row in a related table"
.Vb 1
\&  my $author = $book\->create_related(\*(Aqauthor\*(Aq, { name => \*(AqFred\*(Aq});
.Ve
.SS "Search in a related table"
.IX Subsection "Search in a related table"
Only searches for books named 'Titanic' by the author in \f(CW$author\fR.
.PP
.Vb 1
\&  my $books_rs = $author\->search_related(\*(Aqbooks\*(Aq, { name => \*(AqTitanic\*(Aq });
.Ve
.SS "Delete data in a related table"
.IX Subsection "Delete data in a related table"
Deletes only the book named Titanic by the author in \f(CW$author\fR.
.PP
.Vb 1
\&  $author\->delete_related(\*(Aqbooks\*(Aq, { name => \*(AqTitanic\*(Aq });
.Ve
.SS "Ordering a relationship result set"
.IX Subsection "Ordering a relationship result set"
If you always want a relation to be ordered, you can specify this when you
create the relationship.
.PP
To order \f(CW\*(C`$book\->pages\*(C'\fR by descending page_number, create the relation
as follows:
.PP
.Vb 1
\&  _\|_PACKAGE_\|_\->has_many(\*(Aqpages\*(Aq => \*(AqPage\*(Aq, \*(Aqbook\*(Aq, { order_by => { \-desc => \*(Aqpage_number\*(Aq} } );
.Ve
.SS "Filtering a relationship result set"
.IX Subsection "Filtering a relationship result set"
If you want to get a filtered result set, you can just add to \f(CW$attr\fR as follows:
.PP
.Vb 1
\& _\|_PACKAGE_\|_\->has_many(\*(Aqpages\*(Aq => \*(AqPage\*(Aq, \*(Aqbook\*(Aq, { where => { scrap => 0 } } );
.Ve
.SS "Many-to-many relationship bridges"
.IX Subsection "Many-to-many relationship bridges"
This is straightforward using ManyToMany:
.PP
.Vb 7
\&  package My::User;
\&  use base \*(AqDBIx::Class::Core\*(Aq;
\&  _\|_PACKAGE_\|_\->table(\*(Aquser\*(Aq);
\&  _\|_PACKAGE_\|_\->add_columns(qw/id name/);
\&  _\|_PACKAGE_\|_\->set_primary_key(\*(Aqid\*(Aq);
\&  _\|_PACKAGE_\|_\->has_many(\*(Aquser_address\*(Aq => \*(AqMy::UserAddress\*(Aq, \*(Aquser\*(Aq);
\&  _\|_PACKAGE_\|_\->many_to_many(\*(Aqaddresses\*(Aq => \*(Aquser_address\*(Aq, \*(Aqaddress\*(Aq);
\&
\&  package My::UserAddress;
\&  use base \*(AqDBIx::Class::Core\*(Aq;
\&  _\|_PACKAGE_\|_\->table(\*(Aquser_address\*(Aq);
\&  _\|_PACKAGE_\|_\->add_columns(qw/user address/);
\&  _\|_PACKAGE_\|_\->set_primary_key(qw/user address/);
\&  _\|_PACKAGE_\|_\->belongs_to(\*(Aquser\*(Aq => \*(AqMy::User\*(Aq);
\&  _\|_PACKAGE_\|_\->belongs_to(\*(Aqaddress\*(Aq => \*(AqMy::Address\*(Aq);
\&
\&  package My::Address;
\&  use base \*(AqDBIx::Class::Core\*(Aq;
\&  _\|_PACKAGE_\|_\->table(\*(Aqaddress\*(Aq);
\&  _\|_PACKAGE_\|_\->add_columns(qw/id street town area_code country/);
\&  _\|_PACKAGE_\|_\->set_primary_key(\*(Aqid\*(Aq);
\&  _\|_PACKAGE_\|_\->has_many(\*(Aquser_address\*(Aq => \*(AqMy::UserAddress\*(Aq, \*(Aqaddress\*(Aq);
\&  _\|_PACKAGE_\|_\->many_to_many(\*(Aqusers\*(Aq => \*(Aquser_address\*(Aq, \*(Aquser\*(Aq);
\&
\&  $rs = $user\->addresses(); # get all addresses for a user
\&  $rs = $address\->users(); # get all users for an address
\&
\&  my $address = $user\->add_to_addresses(    # returns a My::Address instance,
\&                                            # NOT a My::UserAddress instance!
\&    {
\&      country => \*(AqUnited Kingdom\*(Aq,
\&      area_code => \*(AqXYZ\*(Aq,
\&      town => \*(AqLondon\*(Aq,
\&      street => \*(AqSesame\*(Aq,
\&    }
\&  );
.Ve
.SS "Relationships across \s-1DB\s0 schemas"
.IX Subsection "Relationships across DB schemas"
Mapping relationships across \s-1DB\s0 schemas
is easy as long as the schemas themselves are all accessible via the same \s-1DBI\s0
connection. In most cases, this means that they are on the same database host
as each other and your connecting database user has the proper permissions to them.
.PP
To accomplish this one only needs to specify the \s-1DB\s0 schema name in the table
declaration, like so...
.PP
.Vb 2
\&  package MyApp::Schema::Result::Artist;
\&  use base qw/DBIx::Class::Core/;
\&
\&  _\|_PACKAGE_\|_\->table(\*(Aqdatabase1.artist\*(Aq); # will use "database1.artist" in FROM clause
\&
\&  _\|_PACKAGE_\|_\->add_columns(qw/ artist_id name /);
\&  _\|_PACKAGE_\|_\->set_primary_key(\*(Aqartist_id\*(Aq);
\&  _\|_PACKAGE_\|_\->has_many(\*(Aqcds\*(Aq => \*(AqMyApp::Schema::Result::Cd\*(Aq);
\&
\&  1;
.Ve
.PP
Whatever string you specify there will be used to build the \*(L"\s-1FROM\*(R"\s0 clause in \s-1SQL\s0
queries.
.PP
The big drawback to this is you now have \s-1DB\s0 schema names hardcoded in your
class files. This becomes especially troublesome if you have multiple instances
of your application to support a change lifecycle (e.g. \s-1DEV, TEST, PROD\s0) and
the \s-1DB\s0 schemas are named based on the environment (e.g. database1_dev).
.PP
However, one can dynamically \*(L"map\*(R" to the proper \s-1DB\s0 schema by overriding the
connection method in your Schema class and
building a renaming facility, like so:
.PP
.Vb 2
\&  package MyApp::Schema;
\&  use Moose;
\&
\&  extends \*(AqDBIx::Class::Schema\*(Aq;
\&
\&  around connection => sub {
\&    my ( $inner, $self, $dsn, $username, $pass, $attr ) = ( shift, @_ );
\&
\&    my $postfix = delete $attr\->{schema_name_postfix};
\&
\&    $inner\->(@_);
\&
\&    if ( $postfix ) {
\&        $self\->append_db_name($postfix);
\&    }
\&  };
\&
\&  sub append_db_name {
\&    my ( $self, $postfix ) = @_;
\&
\&    my @sources_with_db
\&        = grep
\&            { $_\->name =~ /^\ew+\e./mx }
\&            map
\&                { $self\->source($_) }
\&                $self\->sources;
\&
\&    foreach my $source (@sources_with_db) {
\&        my $name = $source\->name;
\&        $name =~ s{^(\ew+)\e.}{${1}${postfix}\e.}mx;
\&
\&        $source\->name($name);
\&    }
\&  }
\&
\&  1;
.Ve
.PP
By overriding the connection
method and extracting a custom option from the provided \e%attr hashref one can
then simply iterate over all the Schema's ResultSources, renaming them as
needed.
.PP
To use this facility, simply add or modify the \e%attr hashref that is passed to
connection, as follows:
.PP
.Vb 9
\&  my $schema
\&    = MyApp::Schema\->connect(
\&      $dsn,
\&      $user,
\&      $pass,
\&      {
\&        schema_name_postfix => \*(Aq_dev\*(Aq
\&        # ... Other options as desired ...
\&      })
.Ve
.PP
Obviously, one could accomplish even more advanced mapping via a hash map or a
callback routine.
.SH "TRANSACTIONS"
.IX Header "TRANSACTIONS"
.SS "Transactions with txn_do"
.IX Subsection "Transactions with txn_do"
As of version 0.04001, there is improved transaction support in
DBIx::Class::Storage and DBIx::Class::Schema.  Here is an
example of the recommended way to use it:
.PP
.Vb 1
\&  my $genus = $schema\->resultset(\*(AqGenus\*(Aq)\->find(12);
\&
\&  my $coderef2 = sub {
\&    $genus\->extinct(1);
\&    $genus\->update;
\&  };
\&
\&  my $coderef1 = sub {
\&    $genus\->add_to_species({ name => \*(Aqtroglodyte\*(Aq });
\&    $genus\->wings(2);
\&    $genus\->update;
\&    $schema\->txn_do($coderef2); # Can have a nested transaction. Only the outer will actualy commit
\&    return $genus\->species;
\&  };
\&
\&  use Try::Tiny;
\&  my $rs;
\&  try {
\&    $rs = $schema\->txn_do($coderef1);
\&  } catch {
\&    # Transaction failed
\&    die "the sky is falling!"           #
\&      if ($_ =~ /Rollback failed/);     # Rollback failed
\&
\&    deal_with_failed_transaction();
\&  };
.Ve
.PP
Note: by default \f(CW\*(C`txn_do\*(C'\fR will re-run the coderef one more time if an
error occurs due to client disconnection (e.g. the server is bounced).
You need to make sure that your coderef can be invoked multiple times
without terrible side effects.
.PP
Nested transactions will work as expected. That is, only the outermost
transaction will actually issue a commit to the \f(CW$dbh\fR, and a rollback
at any level of any transaction will cause the entire nested
transaction to fail.
.SS "Nested transactions and auto-savepoints"
.IX Subsection "Nested transactions and auto-savepoints"
If savepoints are supported by your \s-1RDBMS,\s0 it is possible to achieve true
nested transactions with minimal effort. To enable auto-savepoints via nested
transactions, supply the \f(CW\*(C`auto_savepoint = 1\*(C'\fR connection attribute.
.PP
Here is an example of true nested transactions. In the example, we start a big
task which will create several rows. Generation of data for each row is a
fragile operation and might fail. If we fail creating something, depending on
the type of failure, we want to abort the whole task, or only skip the failed
row.
.PP
.Vb 1
\&  my $schema = MySchema\->connect("dbi:Pg:dbname=my_db");
\&
\&  # Start a transaction. Every database change from here on will only be
\&  # committed into the database if the try block succeeds.
\&  use Try::Tiny;
\&  my $exception;
\&  try {
\&    $schema\->txn_do(sub {
\&      # SQL: BEGIN WORK;
\&
\&      my $job = $schema\->resultset(\*(AqJob\*(Aq)\->create({ name=> \*(Aqbig job\*(Aq });
\&      # SQL: INSERT INTO job ( name) VALUES ( \*(Aqbig job\*(Aq );
\&
\&      for (1..10) {
\&
\&        # Start a nested transaction, which in fact sets a savepoint.
\&        try {
\&          $schema\->txn_do(sub {
\&            # SQL: SAVEPOINT savepoint_0;
\&
\&            my $thing = $schema\->resultset(\*(AqThing\*(Aq)\->create({ job=>$job\->id });
\&            # SQL: INSERT INTO thing ( job) VALUES ( 1 );
\&
\&            if (rand > 0.8) {
\&              # This will generate an error, thus setting $@
\&
\&              $thing\->update({force_fail=>\*(Aqfoo\*(Aq});
\&              # SQL: UPDATE thing SET force_fail = \*(Aqfoo\*(Aq
\&              #      WHERE ( id = 42 );
\&            }
\&          });
\&        } catch {
\&          # SQL: ROLLBACK TO SAVEPOINT savepoint_0;
\&
\&          # There was an error while creating a $thing. Depending on the error
\&          # we want to abort the whole transaction, or only rollback the
\&          # changes related to the creation of this $thing
\&
\&          # Abort the whole job
\&          if ($_ =~ /horrible_problem/) {
\&            print "something horrible happend, aborting job!";
\&            die $_;                # rethrow error
\&          }
\&
\&          # Ignore this $thing, report the error, and continue with the
\&          # next $thing
\&          print "Cannot create thing: $_";
\&        }
\&        # There was no error, so save all changes since the last
\&        # savepoint.
\&
\&        # SQL: RELEASE SAVEPOINT savepoint_0;
\&      }
\&    });
\&  } catch {
\&    $exception = $_;
\&  };
\&
\&  if ($exception) {
\&    # There was an error while handling the $job. Rollback all changes
\&    # since the transaction started, including the already committed
\&    # (\*(Aqreleased\*(Aq) savepoints. There will be neither a new $job nor any
\&    # $thing entry in the database.
\&
\&    # SQL: ROLLBACK;
\&
\&    print "ERROR: $exception\en";
\&  }
\&  else {
\&    # There was no error while handling the $job. Commit all changes.
\&    # Only now other connections can see the newly created $job and
\&    # @things.
\&
\&    # SQL: COMMIT;
\&
\&    print "Ok\en";
\&  }
.Ve
.PP
In this example it might be hard to see where the rollbacks, releases and
commits are happening, but it works just the same as for plain <txn_do>: If
the \f(CW\*(C`try\*(C'\fR\-block around \f(CW\*(C`txn_do\*(C'\fR fails, a rollback is issued. If the \f(CW\*(C`try\*(C'\fR
succeeds, the transaction is committed (or the savepoint released).
.PP
While you can get more fine-grained control using \f(CW\*(C`svp_begin\*(C'\fR, \f(CW\*(C`svp_release\*(C'\fR
and \f(CW\*(C`svp_rollback\*(C'\fR, it is strongly recommended to use \f(CW\*(C`txn_do\*(C'\fR with coderefs.
.SS "Simple Transactions with DBIx::Class::Storage::TxnScopeGuard"
.IX Subsection "Simple Transactions with DBIx::Class::Storage::TxnScopeGuard"
An easy way to use transactions is with
DBIx::Class::Storage::TxnScopeGuard. See \*(L"Automatically creating
related objects\*(R" for an example.
.PP
Note that unlike txn_do, TxnScopeGuard will only make sure the connection is
alive when issuing the \f(CW\*(C`BEGIN\*(C'\fR statement. It will not (and really can not)
retry if the server goes away mid-operations, unlike \f(CW\*(C`txn_do\*(C'\fR.
.SH "SQL"
.IX Header "SQL"
.SS "Creating Schemas From An Existing Database"
.IX Subsection "Creating Schemas From An Existing Database"
DBIx::Class::Schema::Loader will connect to a database and create a
DBIx::Class::Schema and associated sources by examining the database.
.PP
The recommend way of achieving this is to use the dbicdump utility or the
Catalyst helper, as described in
Manual::Intro.
.PP
Alternatively, use the
make_schema_at method:
.PP
.Vb 4
\&  perl \-MDBIx::Class::Schema::Loader=make_schema_at,dump_to_dir:./lib \e
\&    \-e \*(Aqmake_schema_at("My::Schema", \e
\&    { db_schema => \*(Aqmyschema\*(Aq, components => ["InflateColumn::DateTime"] }, \e
\&    [ "dbi:Pg:dbname=foo", "username", "password" ])\*(Aq
.Ve
.PP
This will create a tree of files rooted at \f(CW\*(C`./lib/My/Schema/\*(C'\fR containing source
definitions for all the tables found in the \f(CW\*(C`myschema\*(C'\fR schema in the \f(CW\*(C`foo\*(C'\fR
database.
.SS "Creating \s-1DDL SQL\s0"
.IX Subsection "Creating DDL SQL"
The following functionality requires you to have SQL::Translator
(also known as \*(L"\s-1SQL\s0 Fairy\*(R") installed.
.PP
To create a set of database-specific .sql files for the above schema:
.PP
.Vb 5
\& my $schema = My::Schema\->connect($dsn);
\& $schema\->create_ddl_dir([\*(AqMySQL\*(Aq, \*(AqSQLite\*(Aq, \*(AqPostgreSQL\*(Aq],
\&                        \*(Aq0.1\*(Aq,
\&                        \*(Aq./dbscriptdir/\*(Aq
\&                        );
.Ve
.PP
By default this will create schema files in the current directory, for
MySQL, SQLite and PostgreSQL, using the \f(CW$VERSION\fR from your Schema.pm.
.PP
To create a new database using the schema:
.PP
.Vb 2
\& my $schema = My::Schema\->connect($dsn);
\& $schema\->deploy({ add_drop_table => 1});
.Ve
.PP
To import created .sql files using the mysql client:
.PP
.Vb 1
\&  mysql \-h "host" \-D "database" \-u "user" \-p < My_Schema_1.0_MySQL.sql
.Ve
.PP
To create \f(CW\*(C`ALTER TABLE\*(C'\fR conversion scripts to update a database to a
newer version of your schema at a later point, first set a new
\&\f(CW$VERSION\fR in your Schema file, then:
.PP
.Vb 6
\& my $schema = My::Schema\->connect($dsn);
\& $schema\->create_ddl_dir([\*(AqMySQL\*(Aq, \*(AqSQLite\*(Aq, \*(AqPostgreSQL\*(Aq],
\&                         \*(Aq0.2\*(Aq,
\&                         \*(Aq/dbscriptdir/\*(Aq,
\&                         \*(Aq0.1\*(Aq
\&                         );
.Ve
.PP
This will produce new database-specific .sql files for the new version
of the schema, plus scripts to convert from version 0.1 to 0.2. This
requires that the files for 0.1 as created above are available in the
given directory to diff against.
.SS "Select from dual"
.IX Subsection "Select from dual"
Dummy tables are needed by some databases to allow calling functions
or expressions that aren't based on table content, for examples of how
this applies to various database types, see:
<http://troels.arvin.dk/db/rdbms/#other\-dummy_table>.
.PP
Note: If you're using Oracles dual table don't \fBever\fR do anything
other than a select, if you \s-1CRUD\s0 on your dual table you *will* break
your database.
.PP
Make a table class as you would for any other table
.PP
.Vb 9
\&  package MyAppDB::Dual;
\&  use strict;
\&  use warnings;
\&  use base \*(AqDBIx::Class::Core\*(Aq;
\&  _\|_PACKAGE_\|_\->table("Dual");
\&  _\|_PACKAGE_\|_\->add_columns(
\&    "dummy",
\&    { data_type => "VARCHAR2", is_nullable => 0, size => 1 },
\&  );
.Ve
.PP
Once you've loaded your table class select from it using \f(CW\*(C`select\*(C'\fR
and \f(CW\*(C`as\*(C'\fR instead of \f(CW\*(C`columns\*(C'\fR
.PP
.Vb 5
\&  my $rs = $schema\->resultset(\*(AqDual\*(Aq)\->search(undef,
\&    { select => [ \*(Aqsydate\*(Aq ],
\&      as     => [ \*(Aqnow\*(Aq ]
\&    },
\&  );
.Ve
.PP
All you have to do now is be careful how you access your resultset, the below
will not work because there is no column called 'now' in the Dual table class
.PP
.Vb 4
\&  while (my $dual = $rs\->next) {
\&    print $dual\->now."\en";
\&  }
\&  # Can\*(Aqt locate object method "now" via package "MyAppDB::Dual" at headshot.pl line 23.
.Ve
.PP
You could of course use 'dummy' in \f(CW\*(C`as\*(C'\fR instead of 'now', or \f(CW\*(C`add_columns\*(C'\fR to
your Dual class for whatever you wanted to select from dual, but that's just
silly, instead use \f(CW\*(C`get_column\*(C'\fR
.PP
.Vb 3
\&  while (my $dual = $rs\->next) {
\&    print $dual\->get_column(\*(Aqnow\*(Aq)."\en";
\&  }
.Ve
.PP
Or use \f(CW\*(C`cursor\*(C'\fR
.PP
.Vb 4
\&  my $cursor = $rs\->cursor;
\&  while (my @vals = $cursor\->next) {
\&    print $vals[0]."\en";
\&  }
.Ve
.PP
In case you're going to use this \*(L"trick\*(R" together with \*(L"deploy\*(R" in DBIx::Class::Schema or
\&\*(L"create_ddl_dir\*(R" in DBIx::Class::Schema a table called \*(L"dual\*(R" will be created in your
current schema. This would overlap \*(L"sys.dual\*(R" and you could not fetch \*(L"sysdate\*(R" or
\&\*(L"sequence.nextval\*(R" anymore from dual. To avoid this problem, just tell
SQL::Translator to not create table dual:
.PP
.Vb 5
\&    my $sqlt_args = {
\&        add_drop_table => 1,
\&        parser_args    => { sources => [ grep $_ ne \*(AqDual\*(Aq, schema\->sources ] },
\&    };
\&    $schema\->create_ddl_dir( [qw/Oracle/], undef, \*(Aq./sql\*(Aq, undef, $sqlt_args );
.Ve
.PP
Or use DBIx::Class::ResultClass::HashRefInflator
.PP
.Vb 4
\&  $rs\->result_class(\*(AqDBIx::Class::ResultClass::HashRefInflator\*(Aq);
\&  while ( my $dual = $rs\->next ) {
\&    print $dual\->{now}."\en";
\&  }
.Ve
.PP
Here are some example \f(CW\*(C`select\*(C'\fR conditions to illustrate the different syntax
you could use for doing stuff like
\&\f(CW\*(C`oracles.heavily(nested(functions_can(\*(Aqtake\*(Aq, \*(Aqlots\*(Aq), OF), \*(Aqargs\*(Aq)\*(C'\fR
.PP
.Vb 2
\&  # get a sequence value
\&  select => [ \*(AqA_SEQ.nextval\*(Aq ],
\&
\&  # get create table sql
\&  select => [ { \*(Aqdbms_metadata.get_ddl\*(Aq => [ "\*(AqTABLE\*(Aq", "\*(AqARTIST\*(Aq" ]} ],
\&
\&  # get a random num between 0 and 100
\&  select => [ { "trunc" => [ { "dbms_random.value" => [0,100] } ]} ],
\&
\&  # what year is it?
\&  select => [ { \*(Aqextract\*(Aq => [ \e\*(Aqyear from sysdate\*(Aq ] } ],
\&
\&  # do some math
\&  select => [ {\*(Aqround\*(Aq => [{\*(Aqcos\*(Aq => [ \e\*(Aq180 * 3.14159265359/180\*(Aq ]}]}],
\&
\&  # which day of the week were you born on?
\&  select => [{\*(Aqto_char\*(Aq => [{\*(Aqto_date\*(Aq => [ "\*(Aq25\-DEC\-1980\*(Aq", "\*(Aqdd\-mon\-yyyy\*(Aq" ]}, "\*(Aqday\*(Aq"]}],
\&
\&  # select 16 rows from dual
\&  select   => [ "\*(Aqhello\*(Aq" ],
\&  as       => [ \*(Aqworld\*(Aq ],
\&  group_by => [ \*(Aqcube( 1, 2, 3, 4 )\*(Aq ],
.Ve
.SS "Adding Indexes And Functions To Your \s-1SQL\s0"
.IX Subsection "Adding Indexes And Functions To Your SQL"
Often you will want indexes on columns on your table to speed up searching. To
do this, create a method called \f(CW\*(C`sqlt_deploy_hook\*(C'\fR in the relevant source
class (refer to the advanced
callback system if you wish
to share a hook between multiple sources):
.PP
.Vb 1
\& package My::Schema::Result::Artist;
\&
\& _\|_PACKAGE_\|_\->table(\*(Aqartist\*(Aq);
\& _\|_PACKAGE_\|_\->add_columns(id => { ... }, name => { ... })
\&
\& sub sqlt_deploy_hook {
\&   my ($self, $sqlt_table) = @_;
\&
\&   $sqlt_table\->add_index(name => \*(Aqidx_name\*(Aq, fields => [\*(Aqname\*(Aq]);
\& }
\&
\& 1;
.Ve
.PP
Sometimes you might want to change the index depending on the type of the
database for which \s-1SQL\s0 is being generated:
.PP
.Vb 2
\&  my ($db_type = $sqlt_table\->schema\->translator\->producer_type)
\&    =~ s/^SQL::Translator::Producer:://;
.Ve
.PP
You can also add hooks to the schema level to stop certain tables being
created:
.PP
.Vb 1
\& package My::Schema;
\&
\& ...
\&
\& sub sqlt_deploy_hook {
\&   my ($self, $sqlt_schema) = @_;
\&
\&   $sqlt_schema\->drop_table(\*(Aqtable_name\*(Aq);
\& }
.Ve
.PP
You could also add views, procedures or triggers to the output using
\&\*(L"add_view\*(R" in SQL::Translator::Schema,
\&\*(L"add_procedure\*(R" in SQL::Translator::Schema or
\&\*(L"add_trigger\*(R" in SQL::Translator::Schema.
.SS "Schema versioning"
.IX Subsection "Schema versioning"
The following example shows simplistically how you might use DBIx::Class to
deploy versioned schemas to your customers. The basic process is as follows:
.IP "1." 4
Create a DBIx::Class schema
.IP "2." 4
Save the schema
.IP "3." 4
Deploy to customers
.IP "4." 4
Modify schema to change functionality
.IP "5." 4
Deploy update to customers
.PP
\&\fBCreate a DBIx::Class schema\fR
.PP
This can either be done manually, or generated from an existing database as
described under \*(L"Creating Schemas From An Existing Database\*(R"
.PP
\&\fBSave the schema\fR
.PP
Call \*(L"create_ddl_dir\*(R" in DBIx::Class::Schema as above under \*(L"Creating \s-1DDL SQL\*(R"\s0.
.PP
\&\fBDeploy to customers\fR
.PP
There are several ways you could deploy your schema. These are probably
beyond the scope of this recipe, but might include:
.IP "1." 4
Require customer to apply manually using their \s-1RDBMS.\s0
.IP "2." 4
Package along with your app, making database dump/schema update/tests
all part of your install.
.PP
\&\fBModify the schema to change functionality\fR
.PP
As your application evolves, it may be necessary to modify your schema
to change functionality. Once the changes are made to your schema in
DBIx::Class, export the modified schema and the conversion scripts as
in \*(L"Creating \s-1DDL SQL\*(R"\s0.
.PP
\&\fBDeploy update to customers\fR
.PP
Add the DBIx::Class::Schema::Versioned schema component to your
Schema class. This will add a new table to your database called
\&\f(CW\*(C`dbix_class_schema_vesion\*(C'\fR which will keep track of which version is installed
and warn if the user tries to run a newer schema version than the
database thinks it has.
.PP
Alternatively, you can send the conversion \s-1SQL\s0 scripts to your
customers as above.
.SS "Setting quoting for the generated \s-1SQL\s0"
.IX Subsection "Setting quoting for the generated SQL"
If the database contains column names with spaces and/or reserved words, they
need to be quoted in the \s-1SQL\s0 queries. This is done using:
.PP
.Vb 2
\& $schema\->storage\->sql_maker\->quote_char([ qw/[ ]/] );
\& $schema\->storage\->sql_maker\->name_sep(\*(Aq.\*(Aq);
.Ve
.PP
The first sets the quote characters. Either a pair of matching
brackets, or a \f(CW\*(C`"\*(C'\fR or \f(CW\*(C`\*(Aq\*(C'\fR:
.PP
.Vb 1
\& $schema\->storage\->sql_maker\->quote_char(\*(Aq"\*(Aq);
.Ve
.PP
Check the documentation of your database for the correct quote
characters to use. \f(CW\*(C`name_sep\*(C'\fR needs to be set to allow the \s-1SQL\s0
generator to put the quotes the correct place, and defaults to
\&\f(CW\*(C`.\*(C'\fR if not supplied.
.PP
In most cases you should set these as part of the arguments passed to
\&\*(L"connect\*(R" in DBIx::Class::Schema:
.PP
.Vb 9
\& my $schema = My::Schema\->connect(
\&  \*(Aqdbi:mysql:my_db\*(Aq,
\&  \*(Aqdb_user\*(Aq,
\&  \*(Aqdb_password\*(Aq,
\&  {
\&    quote_char => \*(Aq"\*(Aq,
\&    name_sep   => \*(Aq.\*(Aq
\&  }
\& )
.Ve
.PP
In some cases, quoting will be required for all users of a schema. To enforce
this, you can also overload the \f(CW\*(C`connection\*(C'\fR method for your schema class:
.PP
.Vb 7
\& sub connection {
\&     my $self = shift;
\&     my $rv = $self\->next::method( @_ );
\&     $rv\->storage\->sql_maker\->quote_char([ qw/[ ]/ ]);
\&     $rv\->storage\->sql_maker\->name_sep(\*(Aq.\*(Aq);
\&     return $rv;
\& }
.Ve
.SS "Working with PostgreSQL array types"
.IX Subsection "Working with PostgreSQL array types"
You can also assign values to PostgreSQL array columns by passing array
references in the \f(CW\*(C`\e%columns\*(C'\fR (\f(CW\*(C`\e%vals\*(C'\fR) hashref of the
\&\*(L"create\*(R" in DBIx::Class::ResultSet and \*(L"update\*(R" in DBIx::Class::Row family of
methods:
.PP
.Vb 3
\&  $resultset\->create({
\&    numbers => [1, 2, 3]
\&  });
\&
\&  $result\->update(
\&    {
\&      numbers => [1, 2, 3]
\&    },
\&    {
\&      day => \*(Aq2008\-11\-24\*(Aq
\&    }
\&  );
.Ve
.PP
In conditions (e.g. \f(CW\*(C`\e%cond\*(C'\fR in the \*(L"search\*(R" in DBIx::Class::ResultSet family of
methods) you cannot directly use array references (since this is interpreted as
a list of values to be \f(CW\*(C`OR\*(C'\fRed), but you can use the following syntax to force
passing them as bind values:
.PP
.Vb 5
\&  $resultset\->search(
\&    {
\&      numbers => \e[ \*(Aq= ?\*(Aq, [numbers => [1, 2, 3]] ]
\&    }
\&  );
.Ve
.PP
See \*(L"array_datatypes\*(R" in SQL::Abstract and \*(L"Literal \s-1SQL\s0 with
placeholders and bind values (subqueries)\*(R" in SQL::Abstract for more explanation. Note that
DBIx::Class sets \*(L"bindtype\*(R" in SQL::Abstract to \f(CW\*(C`columns\*(C'\fR, so you must pass
the bind values (the \f(CW\*(C`[1, 2, 3]\*(C'\fR arrayref in the above example) wrapped in
arrayrefs together with the column name, like this:
\&\f(CW\*(C`[column_name => value]\*(C'\fR.
.SS "Formatting DateTime objects in queries"
.IX Subsection "Formatting DateTime objects in queries"
To ensure \f(CW\*(C`WHERE\*(C'\fR conditions containing DateTime arguments are properly
formatted to be understood by your \s-1RDBMS,\s0 you must use the \f(CW\*(C`DateTime\*(C'\fR
formatter returned by \*(L"datetime_parser\*(R" in DBIx::Class::Storage::DBI to format
any DateTime objects you pass to search
conditions. Any Storage object attached to your
Schema provides a correct \f(CW\*(C`DateTime\*(C'\fR formatter, so
all you have to do is:
.PP
.Vb 11
\&  my $dtf = $schema\->storage\->datetime_parser;
\&  my $rs = $schema\->resultset(\*(Aqusers\*(Aq)\->search(
\&    {
\&      signup_date => {
\&        \-between => [
\&          $dtf\->format_datetime($dt_start),
\&          $dtf\->format_datetime($dt_end),
\&        ],
\&      }
\&    },
\&  );
.Ve
.PP
Without doing this the query will contain the simple stringification of the
\&\f(CW\*(C`DateTime\*(C'\fR object, which almost never matches the \s-1RDBMS\s0 expectations.
.PP
This kludge is necessary only for conditions passed to
\&\*(L"search\*(R" in DBIx::Class::ResultSet, whereas
create,
find,
\&\*(L"update\*(R" in DBIx::Class::Row (but not \*(L"update\*(R" in DBIx::Class::ResultSet) are all
DBIx::Class::InflateColumn\-aware and will do the right thing when supplied
an inflated \f(CW\*(C`DateTime\*(C'\fR object.
.SS "Using Unicode"
.IX Subsection "Using Unicode"
When using unicode character data there are two alternatives \-
either your database supports unicode characters (including setting
the utf8 flag on the returned string), or you need to encode/decode
data appropriately each time a string field is inserted into or
retrieved from the database. It is better to avoid
encoding/decoding data and to use your database's own unicode
capabilities if at all possible.
.PP
The DBIx::Class::UTF8Columns component handles storing selected
unicode columns in a database that does not directly support
unicode. If used with a database that does correctly handle unicode
then strange and unexpected data corrupt \fBwill\fR occur.
.PP
The Catalyst Wiki Unicode page at
<http://wiki.catalystframework.org/wiki/tutorialsandhowtos/using_unicode>
has additional information on the use of Unicode with Catalyst and
DBIx::Class.
.PP
The following databases do correctly handle unicode data:\-
.PP
\fIMySQL\fR
.IX Subsection "MySQL"
.PP
MySQL supports unicode, and will correctly flag utf8 data from the
database if the \f(CW\*(C`mysql_enable_utf8\*(C'\fR is set in the connect options.
.PP
.Vb 3
\&  my $schema = My::Schema\->connection(\*(Aqdbi:mysql:dbname=test\*(Aq,
\&                                      $user, $pass,
\&                                      { mysql_enable_utf8 => 1} );
.Ve
.PP
When set, a data retrieved from a textual column type (char,
varchar, etc) will have the \s-1UTF\-8\s0 flag turned on if necessary. This
enables character semantics on that string. You will also need to
ensure that your database / table / column is configured to use
\&\s-1UTF8.\s0 See Chapter 10 of the mysql manual for details.
.PP
See DBD::mysql for further details.
.PP
\fIOracle\fR
.IX Subsection "Oracle"
.PP
Information about Oracle support for unicode can be found in
\&\*(L"Unicode\*(R" in DBD::Oracle.
.PP
\fIPostgreSQL\fR
.IX Subsection "PostgreSQL"
.PP
PostgreSQL supports unicode if the character set is correctly set
at database creation time. Additionally the \f(CW\*(C`pg_enable_utf8\*(C'\fR
should be set to ensure unicode data is correctly marked.
.PP
.Vb 3
\&  my $schema = My::Schema\->connection(\*(Aqdbi:Pg:dbname=test\*(Aq,
\&                                      $user, $pass,
\&                                      { pg_enable_utf8 => 1} );
.Ve
.PP
Further information can be found in DBD::Pg.
.PP
\fISQLite\fR
.IX Subsection "SQLite"
.PP
SQLite version 3 and above natively use unicode internally. To
correctly mark unicode strings taken from the database, the
\&\f(CW\*(C`sqlite_unicode\*(C'\fR flag should be set at connect time (in versions
of DBD::SQLite prior to 1.27 this attribute was named
\&\f(CW\*(C`unicode\*(C'\fR).
.PP
.Vb 3
\&  my $schema = My::Schema\->connection(\*(Aqdbi:SQLite:/tmp/test.db\*(Aq,
\&                                      \*(Aq\*(Aq, \*(Aq\*(Aq,
\&                                      { sqlite_unicode => 1} );
.Ve
.SH "BOOTSTRAPPING/MIGRATING"
.IX Header "BOOTSTRAPPING/MIGRATING"
.SS "Easy migration from class-based to schema-based setup"
.IX Subsection "Easy migration from class-based to schema-based setup"
You want to start using the schema-based approach to DBIx::Class
(see \*(L"Setting it up manually\*(R" in DBIx::Class::Manual::Intro), but have an
established class-based setup with lots of existing classes that you don't
want to move by hand. Try this nifty script instead:
.PP
.Vb 2
\&  use MyDB;
\&  use SQL::Translator;
\&
\&  my $schema = MyDB\->schema_instance;
\&
\&  my $translator           =  SQL::Translator\->new(
\&      debug                => $debug          ||  0,
\&      trace                => $trace          ||  0,
\&      no_comments          => $no_comments    ||  0,
\&      show_warnings        => $show_warnings  ||  0,
\&      add_drop_table       => $add_drop_table ||  0,
\&      validate             => $validate       ||  0,
\&      parser_args          => {
\&         \*(AqDBIx::Schema\*(Aq    => $schema,
\&                              },
\&      producer_args   => {
\&          \*(Aqprefix\*(Aq         => \*(AqMy::Schema\*(Aq,
\&                         },
\&  );
\&
\&  $translator\->parser(\*(AqSQL::Translator::Parser::DBIx::Class\*(Aq);
\&  $translator\->producer(\*(AqSQL::Translator::Producer::DBIx::Class::File\*(Aq);
\&
\&  my $output = $translator\->translate(@args) or die
\&          "Error: " . $translator\->error;
\&
\&  print $output;
.Ve
.PP
You could use Module::Find to search for all subclasses in the MyDB::*
namespace, which is currently left as an exercise for the reader.
.SH "OVERLOADING METHODS"
.IX Header "OVERLOADING METHODS"
DBIx::Class uses the Class::C3 package, which provides for redispatch of
method calls, useful for things like default values and triggers. You have to
use calls to \f(CW\*(C`next::method\*(C'\fR to overload methods. More information on using
Class::C3 with DBIx::Class can be found in
DBIx::Class::Manual::Component.
.SS "Setting default values for a row"
.IX Subsection "Setting default values for a row"
It's as simple as overriding the \f(CW\*(C`new\*(C'\fR method.  Note the use of
\&\f(CW\*(C`next::method\*(C'\fR.
.PP
.Vb 2
\&  sub new {
\&    my ( $class, $attrs ) = @_;
\&
\&    $attrs\->{foo} = \*(Aqbar\*(Aq unless defined $attrs\->{foo};
\&
\&    my $new = $class\->next::method($attrs);
\&
\&    return $new;
\&  }
.Ve
.PP
For more information about \f(CW\*(C`next::method\*(C'\fR, look in the Class::C3
documentation. See also DBIx::Class::Manual::Component for more
ways to write your own base classes to do this.
.PP
People looking for ways to do \*(L"triggers\*(R" with DBIx::Class are probably
just looking for this.
.SS "Changing one field whenever another changes"
.IX Subsection "Changing one field whenever another changes"
For example, say that you have three columns, \f(CW\*(C`id\*(C'\fR, \f(CW\*(C`number\*(C'\fR, and
\&\f(CW\*(C`squared\*(C'\fR.  You would like to make changes to \f(CW\*(C`number\*(C'\fR and have
\&\f(CW\*(C`squared\*(C'\fR be automagically set to the value of \f(CW\*(C`number\*(C'\fR squared.
You can accomplish this by wrapping the \f(CW\*(C`number\*(C'\fR accessor with the \f(CW\*(C`around\*(C'\fR
method modifier, available through either Class::Method::Modifiers,
Moose or Moose-like modules):
.PP
.Vb 2
\&  around number => sub {
\&    my ($orig, $self) = (shift, shift);
\&
\&    if (@_) {
\&      my $value = $_[0];
\&      $self\->squared( $value * $value );
\&    }
\&
\&    $self\->$orig(@_);
\&  };
.Ve
.PP
Note that the hard work is done by the call to \f(CW\*(C`$self\->$orig\*(C'\fR, which
redispatches your call to store_column in the superclass(es).
.PP
Generally, if this is a calculation your database can easily do, try
and avoid storing the calculated value, it is safer to calculate when
needed, than rely on the data being in sync.
.SS "Automatically creating related objects"
.IX Subsection "Automatically creating related objects"
You might have a class \f(CW\*(C`Artist\*(C'\fR which has many \f(CW\*(C`CD\*(C'\fRs.  Further, you
want to create a \f(CW\*(C`CD\*(C'\fR object every time you insert an \f(CW\*(C`Artist\*(C'\fR object.
You can accomplish this by overriding \f(CW\*(C`insert\*(C'\fR on your objects:
.PP
.Vb 6
\&  sub insert {
\&    my ( $self, @args ) = @_;
\&    $self\->next::method(@args);
\&    $self\->create_related (\*(Aqcds\*(Aq, \e%initial_cd_data );
\&    return $self;
\&  }
.Ve
.PP
If you want to wrap the two inserts in a transaction (for consistency,
an excellent idea), you can use the awesome
DBIx::Class::Storage::TxnScopeGuard:
.PP
.Vb 2
\&  sub insert {
\&    my ( $self, @args ) = @_;
\&
\&    my $guard = $self\->result_source\->schema\->txn_scope_guard;
\&
\&    $self\->next::method(@args);
\&    $self\->create_related (\*(Aqcds\*(Aq, \e%initial_cd_data );
\&
\&    $guard\->commit;
\&
\&    return $self
\&  }
.Ve
.SS "Wrapping/overloading a column accessor"
.IX Subsection "Wrapping/overloading a column accessor"
\&\fBProblem:\fR
.PP
Say you have a table \*(L"Camera\*(R" and want to associate a description
with each camera. For most cameras, you'll be able to generate the description from
the other columns. However, in a few special cases you may want to associate a
custom description with a camera.
.PP
\&\fBSolution:\fR
.PP
In your database schema, define a description field in the \*(L"Camera\*(R" table that
can contain text and null values.
.PP
In \s-1DBIC,\s0 we'll overload the column accessor to provide a sane default if no
custom description is defined. The accessor will either return or generate the
description, depending on whether the field is null or not.
.PP
First, in your \*(L"Camera\*(R" schema class, define the description field as follows:
.PP
.Vb 1
\&  _\|_PACKAGE_\|_\->add_columns(description => { accessor => \*(Aq_description\*(Aq });
.Ve
.PP
Next, we'll define the accessor-wrapper subroutine:
.PP
.Vb 2
\&  sub description {
\&      my $self = shift;
\&
\&      # If there is an update to the column, we\*(Aqll let the original accessor
\&      # deal with it.
\&      return $self\->_description(@_) if @_;
\&
\&      # Fetch the column value.
\&      my $description = $self\->_description;
\&
\&      # If there\*(Aqs something in the description field, then just return that.
\&      return $description if defined $description && length $descripton;
\&
\&      # Otherwise, generate a description.
\&      return $self\->generate_description;
\&  }
.Ve
.SH "DEBUGGING AND PROFILING"
.IX Header "DEBUGGING AND PROFILING"
.SS "DBIx::Class objects with Data::Dumper"
.IX Subsection "DBIx::Class objects with Data::Dumper"
Data::Dumper can be a very useful tool for debugging, but sometimes it can
be hard to find the pertinent data in all the data it can generate.
Specifically, if one naively tries to use it like so,
.PP
.Vb 1
\&  use Data::Dumper;
\&
\&  my $cd = $schema\->resultset(\*(AqCD\*(Aq)\->find(1);
\&  print Dumper($cd);
.Ve
.PP
several pages worth of data from the \s-1CD\s0 object's schema and result source will
be dumped to the screen. Since usually one is only interested in a few column
values of the object, this is not very helpful.
.PP
Luckily, it is possible to modify the data before Data::Dumper outputs
it. Simply define a hook that Data::Dumper will call on the object before
dumping it. For example,
.PP
.Vb 1
\&  package My::DB::CD;
\&
\&  sub _dumper_hook {
\&    $_[0] = bless {
\&      %{ $_[0] },
\&      result_source => undef,
\&    }, ref($_[0]);
\&  }
\&
\&  [...]
\&
\&  use Data::Dumper;
\&
\&  local $Data::Dumper::Freezer = \*(Aq_dumper_hook\*(Aq;
\&
\&  my $cd = $schema\->resultset(\*(AqCD\*(Aq)\->find(1);
\&  print Dumper($cd);
\&         # dumps $cd without its ResultSource
.Ve
.PP
If the structure of your schema is such that there is a common base class for
all your table classes, simply put a method similar to \f(CW\*(C`_dumper_hook\*(C'\fR in the
base class and set \f(CW$Data::Dumper::Freezer\fR to its name and Data::Dumper
will automagically clean up your data before printing it. See
\&\*(L"\s-1EXAMPLES\*(R"\s0 in Data::Dumper for more information.
.SS "Profiling"
.IX Subsection "Profiling"
When you enable DBIx::Class::Storage's debugging it prints the \s-1SQL\s0
executed as well as notifications of query completion and transaction
begin/commit.  If you'd like to profile the \s-1SQL\s0 you can subclass the
DBIx::Class::Storage::Statistics class and write your own profiling
mechanism:
.PP
.Vb 2
\&  package My::Profiler;
\&  use strict;
\&
\&  use base \*(AqDBIx::Class::Storage::Statistics\*(Aq;
\&
\&  use Time::HiRes qw(time);
\&
\&  my $start;
\&
\&  sub query_start {
\&    my $self = shift();
\&    my $sql = shift();
\&    my @params = @_;
\&
\&    $self\->print("Executing $sql: ".join(\*(Aq, \*(Aq, @params)."\en");
\&    $start = time();
\&  }
\&
\&  sub query_end {
\&    my $self = shift();
\&    my $sql = shift();
\&    my @params = @_;
\&
\&    my $elapsed = sprintf("%0.4f", time() \- $start);
\&    $self\->print("Execution took $elapsed seconds.\en");
\&    $start = undef;
\&  }
\&
\&  1;
.Ve
.PP
You can then install that class as the debugging object:
.PP
.Vb 2
\&  _\|_PACKAGE_\|_\->storage\->debugobj(new My::Profiler());
\&  _\|_PACKAGE_\|_\->storage\->debug(1);
.Ve
.PP
A more complicated example might involve storing each execution of \s-1SQL\s0 in an
array:
.PP
.Vb 4
\&  sub query_end {
\&    my $self = shift();
\&    my $sql = shift();
\&    my @params = @_;
\&
\&    my $elapsed = time() \- $start;
\&    push(@{ $calls{$sql} }, {
\&        params => \e@params,
\&        elapsed => $elapsed
\&    });
\&  }
.Ve
.PP
You could then create average, high and low execution times for an \s-1SQL\s0
statement and dig down to see if certain parameters cause aberrant behavior.
You might want to check out DBIx::Class::QueryLog as well.
.SH "IMPROVING PERFORMANCE"
.IX Header "IMPROVING PERFORMANCE"
.IP "\(bu" 4
Install Class::XSAccessor to speed up Class::Accessor::Grouped.
.IP "\(bu" 4
On Perl 5.8 install Class::C3::XS.
.IP "\(bu" 4
prefetch relationships, where possible. See
\&\*(L"Using joins and prefetch\*(R".
.IP "\(bu" 4
Use populate in void context to insert data
when you don't need the resulting result objects,
if possible, but see the caveats.
.Sp
When inserting many rows, for best results, populate a large number of rows at a
time, but not so large that the table is locked for an unacceptably long time.
.Sp
If using create instead, use a transaction and
commit every \f(CW\*(C`X\*(C'\fR rows; where \f(CW\*(C`X\*(C'\fR gives you the best performance without
locking the table for too long.
.IP "\(bu" 4
When selecting many rows, if you don't need full-blown DBIx::Class::Row
objects, consider using DBIx::Class::ResultClass::HashRefInflator.
.IP "\(bu" 4
See also \*(L"\s-1STARTUP SPEED\*(R"\s0 and \*(L"\s-1MEMORY USAGE\*(R"\s0 in this document.
.SH "STARTUP SPEED"
.IX Header "STARTUP SPEED"
DBIx::Class programs can have a significant startup delay
as the \s-1ORM\s0 loads all the relevant classes. This section examines
techniques for reducing the startup delay.
.PP
These tips are listed in order of decreasing effectiveness \- so the
first tip, if applicable, should have the greatest effect on your
application.
.SS "Statically Define Your Schema"
.IX Subsection "Statically Define Your Schema"
If you are using
DBIx::Class::Schema::Loader to build the
classes dynamically based on the database schema then there will be a
significant startup delay.
.PP
For production use a statically defined schema (which can be generated
using DBIx::Class::Schema::Loader to dump
the database schema once \- see
make_schema_at and
dump_directory for more
details on creating static schemas from a database).
.SS "Move Common Startup into a Base Class"
.IX Subsection "Move Common Startup into a Base Class"
Typically DBIx::Class result classes start off with
.PP
.Vb 2
\&    use base qw/DBIx::Class::Core/;
\&    _\|_PACKAGE_\|_\->load_components(qw/InflateColumn::DateTime/);
.Ve
.PP
If this preamble is moved into a common base class:\-
.PP
.Vb 1
\&    package MyDBICbase;
\&
\&    use base qw/DBIx::Class::Core/;
\&    _\|_PACKAGE_\|_\->load_components(qw/InflateColumn::DateTime/);
\&    1;
.Ve
.PP
and each result class then uses this as a base:\-
.PP
.Vb 1
\&    use base qw/MyDBICbase/;
.Ve
.PP
then the load_components is only performed once, which can result in a
considerable startup speedup for schemas with many classes.
.SS "Explicitly List Schema Result Classes"
.IX Subsection "Explicitly List Schema Result Classes"
The schema class will normally contain
.PP
.Vb 1
\&    _\|_PACKAGE_\|_\->load_classes();
.Ve
.PP
to load the result classes. This will use Module::Find
to find and load the appropriate modules. Explicitly defining the
classes you wish to load will remove the overhead of
Module::Find and the related directory operations:
.PP
.Vb 1
\&    _\|_PACKAGE_\|_\->load_classes(qw/ CD Artist Track /);
.Ve
.PP
If you are instead using the load_namespaces
syntax to load the appropriate classes there is not a direct alternative
avoiding Module::Find.
.SH "MEMORY USAGE"
.IX Header "MEMORY USAGE"
.SS "Cached statements"
.IX Subsection "Cached statements"
DBIx::Class normally caches all statements with \fIprepare_cached()\fR.
This is normally a good idea, but if too many statements are cached, the database may use too much
memory and may eventually run out and fail entirely.  If you suspect this may be the case, you may want
to examine \s-1DBI\s0's CachedKids hash:
.PP
.Vb 4
\&    # print all currently cached prepared statements
\&    print for keys %{$schema\->storage\->dbh\->{CachedKids}};
\&    # get a count of currently cached prepared statements
\&    my $count = scalar keys %{$schema\->storage\->dbh\->{CachedKids}};
.Ve
.PP
If it's appropriate, you can simply clear these statements, automatically deallocating them in the
database:
.PP
.Vb 2
\&    my $kids = $schema\->storage\->dbh\->{CachedKids};
\&    delete @{$kids}{keys %$kids} if scalar keys %$kids > 100;
.Ve
.PP
But what you probably want is to expire unused statements and not those that are used frequently.
You can accomplish this with Tie::Cache or Tie::Cache::LRU:
.PP
.Vb 5
\&    use Tie::Cache;
\&    use DB::Main;
\&    my $schema = DB::Main\->connect($dbi_dsn, $user, $pass, {
\&        on_connect_do => sub { tie %{shift\->_dbh\->{CachedKids}}, \*(AqTie::Cache\*(Aq, 100 },
\&    });
.Ve
